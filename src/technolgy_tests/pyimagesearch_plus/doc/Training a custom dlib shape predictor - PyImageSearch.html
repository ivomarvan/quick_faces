<!DOCTYPE html>
<html lang="en-US">
<head >
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Training a custom dlib shape predictor - PyImageSearch</title>

<!-- This site is optimized with the Yoast SEO plugin v13.0 - https://yoast.com/wordpress/plugins/seo/ -->
<meta name="description" content="In this tutorial, you will learn how to train your own custom dlib shape predictor. You&#039;ll then learn how to take your trained dlib shape predictor and use it to predict landmarks on input images and real-time video streams."/>
<meta name="robots" content="max-snippet:-1, max-image-preview:large, max-video-preview:-1"/>
<link rel="canonical" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Training a custom dlib shape predictor - PyImageSearch" />
<meta property="og:description" content="In this tutorial, you will learn how to train your own custom dlib shape predictor. You&#039;ll then learn how to take your trained dlib shape predictor and use it to predict landmarks on input images and real-time video streams." />
<meta property="og:url" content="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/" />
<meta property="og:site_name" content="PyImageSearch" />
<meta property="article:tag" content="dlib" />
<meta property="article:tag" content="facial landmarks" />
<meta property="article:tag" content="landmark predictors" />
<meta property="article:tag" content="shape predictors" />
<meta property="article:section" content="dlib" />
<meta property="article:published_time" content="2019-12-16T15:00:59+00:00" />
<meta property="article:modified_time" content="2020-04-18T18:32:25+00:00" />
<meta property="og:updated_time" content="2020-04-18T18:32:25+00:00" />
<meta property="og:image" content="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_featured.jpg" />
<meta property="og:image:secure_url" content="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_featured.jpg" />
<meta property="og:image:width" content="600" />
<meta property="og:image:height" content="410" />
<script type='application/ld+json' class='yoast-schema-graph yoast-schema-graph--main'>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.pyimagesearch.com/#website","url":"https://www.pyimagesearch.com/","name":"PyImageSearch","description":"You can master Computer Vision, Deep Learning, and OpenCV - PyImageSearch","potentialAction":{"@type":"SearchAction","target":"https://www.pyimagesearch.com/?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#primaryimage","url":"https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_featured.jpg","width":600,"height":410},{"@type":"WebPage","@id":"https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#webpage","url":"https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/","inLanguage":"en-US","name":"Training a custom dlib shape predictor - PyImageSearch","isPartOf":{"@id":"https://www.pyimagesearch.com/#website"},"primaryImageOfPage":{"@id":"https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#primaryimage"},"datePublished":"2019-12-16T15:00:59+00:00","dateModified":"2020-04-18T18:32:25+00:00","author":{"@id":"https://www.pyimagesearch.com/#/schema/person/5901b399e2f20b986362a00636181cca"},"description":"In this tutorial, you will learn how to train your own custom dlib shape predictor. You'll then learn how to take your trained dlib shape predictor and use it to predict landmarks on input images and real-time video streams."},{"@type":["Person"],"@id":"https://www.pyimagesearch.com/#/schema/person/5901b399e2f20b986362a00636181cca","name":"Adrian Rosebrock","image":{"@type":"ImageObject","@id":"https://www.pyimagesearch.com/#authorlogo","url":"https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&d=mm&r=g","caption":"Adrian Rosebrock"},"description":"Hi there, I\u2019m Adrian Rosebrock, PhD. All too often I see developers, students, and researchers wasting their time, studying the wrong things, and generally struggling to get started with Computer Vision, Deep Learning, and OpenCV. I created this website to show you what I believe is the best possible way to get your start.","sameAs":[]}]}</script>
<!-- / Yoast SEO plugin. -->

<link rel='dns-prefetch' href='//www.google.com' />
<link rel='dns-prefetch' href='//a.opmnstr.com' />
<link rel='dns-prefetch' href='//use.typekit.net' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="PyImageSearch &raquo; Feed" href="https://www.pyimagesearch.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="PyImageSearch &raquo; Comments Feed" href="https://www.pyimagesearch.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="PyImageSearch &raquo; Training a custom dlib shape predictor Comments Feed" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/feed/" />
<link rel="stylesheet" href="https://www.pyimagesearch.com/wp-content/cache/minify/41985.css" media="all" />









<link rel='stylesheet' id='pyimagesearch-fonts-css'  href='https://use.typekit.net/aay3jsp.css?ver=5.3.2' type='text/css' media='all' />
<link rel="stylesheet" href="https://www.pyimagesearch.com/wp-content/cache/minify/b9c43.css" media="all" />



<script src="https://www.pyimagesearch.com/wp-content/cache/minify/ad8a3.js"></script>


<script type='text/javascript' data-cfasync="false" id="omapi-script" async="async" src='https://a.opmnstr.com/app/js/api.min.js'></script>
<link rel='https://api.w.org/' href='https://www.pyimagesearch.com/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://www.pyimagesearch.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://www.pyimagesearch.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 5.3.2" />
<link rel='shortlink' href='https://www.pyimagesearch.com/?p=11908' />
<link rel="alternate" type="application/json+oembed" href="https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.pyimagesearch.com%2F2019%2F12%2F16%2Ftraining-a-custom-dlib-shape-predictor%2F" />
<link rel="alternate" type="text/xml+oembed" href="https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.pyimagesearch.com%2F2019%2F12%2F16%2Ftraining-a-custom-dlib-shape-predictor%2F&#038;format=xml" />
    <script type="text/javascript">
        var ajaxurl = 'https://www.pyimagesearch.com/wp-admin/admin-ajax.php';
    </script>
<link rel="pingback" href="https://www.pyimagesearch.com/xmlrpc.php" />
<style type="text/css">
/* <![CDATA[ */
img.latex { vertical-align: middle; border: none; }
/* ]]> */
</style>
		<style type="text/css" id="wp-custom-css">
			.grecaptcha-badge {
    display: none !important;
}

img.latex {
	margin: 0!important;
	display: inline!important;
}

.entry-content > .aligncenter {
	margin-left: auto;
	margin-right: auto;
} 

.page-template-page_success_stories .success-story-all .success-story-item {
	break-inside: avoid;
	float: none;
}		</style>
		</head>
<body class="post-template-default single single-post postid-11908 single-format-standard wp-embed-responsive header-full-width content-sidebar genesis-breadcrumbs-hidden genesis-footer-widgets-visible"><div class="site-container"><ul class="genesis-skip-link"><li><a href="#genesis-nav-primary" class="screen-reader-shortcut"> Skip to primary navigation</a></li><li><a href="#genesis-content" class="screen-reader-shortcut"> Skip to main content</a></li><li><a href="#genesis-sidebar-primary" class="screen-reader-shortcut"> Skip to primary sidebar</a></li><li><a href="#genesis-footer-widgets" class="screen-reader-shortcut"> Skip to footer</a></li></ul><header class="site-header"><div class="wrap"><div class="title-area"><p class="site-title"><a href="https://www.pyimagesearch.com/">PyImageSearch</a></p><p class="site-description">You can master Computer Vision, Deep Learning, and OpenCV - PyImageSearch</p></div><nav class="nav-secondary" aria-label="Secondary"><div class="wrap"><ul id="menu-header-secondary" class="menu genesis-nav-menu menu-secondary"><li id="menu-item-15978" class="menu-item"><a href="https://www.pyimagesearch.com/opencv-tutorials-resources-guides/"><span >OpenCV Install Guides</span></a></li>
<li id="menu-item-12816" class="menu-item"><a href="https://www.pyimagesearch.com/about/"><span >About</span></a></li>
<li id="menu-item-12817" class="menu-item"><a href="https://www.pyimagesearch.com/faqs/"><span >FAQ</span></a></li>
<li id="menu-item-12818" class="menu-item"><a href="https://www.pyimagesearch.com/contact/"><span >Contact</span></a></li>
</ul></div></nav><div class="main-nav-wrap"><nav class="nav-primary" aria-label="Main" id="genesis-nav-primary"><ul id="menu-main-menu" class="menu genesis-nav-menu menu-primary"><li id="menu-item-11459" class="menu-item"><a href="https://www.pyimagesearch.com/start-here/"><span >Get Started</span></a></li>
<li id="menu-item-10696" class="is-topics menu-item menu-item-has-children"><a href="/topics/"><span >Topics</span></a><span class="submenu-expand" tabindex="-1"><svg class="svg-icon" width="16" height="16" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M151.5 347.8L3.5 201c-4.7-4.7-4.7-12.3 0-17l19.8-19.8c4.7-4.7 12.3-4.7 17 0L160 282.7l119.7-118.5c4.7-4.7 12.3-4.7 17 0l19.8 19.8c4.7 4.7 4.7 12.3 0 17l-148 146.8c-4.7 4.7-12.3 4.7-17 0z"/></svg></span>
<ul class="sub-menu">
	<li id="menu-item-10698" class="has-icon has-icon--deep-learning menu-item"><a href="https://www.pyimagesearch.com/category/deep-learning/"><span >Deep Learning</span></a></li>
	<li id="menu-item-10699" class="has-icon has-icon--dlib menu-item current-post-ancestor current-menu-parent current-post-parent"><a href="https://www.pyimagesearch.com/category/dlib/"><span >Dlib Library</span></a></li>
	<li id="menu-item-10700" class="has-icon has-icon--iot menu-item"><a href="https://www.pyimagesearch.com/category/embedded/"><span >Embedded/IoT and Computer Vision</span></a></li>
	<li id="menu-item-10701" class="has-icon has-icon--face menu-item current-post-ancestor current-menu-parent current-post-parent"><a href="https://www.pyimagesearch.com/category/faces/"><span >Face Applications</span></a></li>
	<li id="menu-item-10702" class="has-icon has-icon--image menu-item"><a href="https://www.pyimagesearch.com/category/image-processing/"><span >Image Processing</span></a></li>
	<li id="menu-item-10703" class="has-icon has-icon--interviews menu-item"><a href="https://www.pyimagesearch.com/category/interviews/"><span >Interviews</span></a></li>
	<li id="menu-item-10704" class="has-icon has-icon--keras menu-item"><a href="https://www.pyimagesearch.com/category/keras-and-tensorflow/"><span >Keras and TensorFlow</span></a></li>
	<li id="menu-item-10705" class="has-icon has-icon--ml menu-item"><a href="https://www.pyimagesearch.com/category/machine-learning/"><span >Machine Learning and Computer Vision</span></a></li>
	<li id="menu-item-10706" class="has-icon has-icon--medical menu-item"><a href="https://www.pyimagesearch.com/category/medical/"><span >Medical Computer Vision</span></a></li>
	<li id="menu-item-10707" class="has-icon has-icon--ocr menu-item"><a href="https://www.pyimagesearch.com/category/optical-character-recognition-ocr/"><span >Optical Character Recognition (OCR)</span></a></li>
	<li id="menu-item-10708" class="has-icon has-icon--object-detection menu-item"><a href="https://www.pyimagesearch.com/category/object-detection/"><span >Object Detection</span></a></li>
	<li id="menu-item-10709" class="has-icon has-icon--object-tracking menu-item"><a href="https://www.pyimagesearch.com/category/object-tracking/"><span >Object Tracking</span></a></li>
	<li id="menu-item-10711" class="has-icon has-icon--opencv menu-item"><a href="https://www.pyimagesearch.com/category/opencv/"><span >OpenCV Tutorials</span></a></li>
	<li id="menu-item-10710" class="has-icon has-icon--pi menu-item"><a href="https://www.pyimagesearch.com/category/raspberry-pi/"><span >Raspberry Pi</span></a></li>
</ul>
</li>
<li id="menu-item-12831" class="menu-item"><a href="https://www.pyimagesearch.com/books-and-courses/"><span >Books and Courses</span></a></li>
<li id="menu-item-15979" class="menu-item"><a href="https://www.pyimagesearch.com/pyimagesearch-reviews-testimonials/"><span >Student Success Stories</span></a></li>
<li id="menu-item-12845" class="menu-item current_page_parent"><a href="https://www.pyimagesearch.com/blog/"><span >Blog</span></a></li>
<li id="menu-item-2619" class="mobile-only menu-item"><a href="https://www.pyimagesearch.com/about/"><span >About</span></a></li>
<li id="menu-item-10258" class="mobile-only menu-item"><a href="https://www.pyimagesearch.com/faqs/"><span >FAQ</span></a></li>
<li id="menu-item-6744" class="mobile-only menu-item"><a href="https://www.pyimagesearch.com/contact/"><span >Contact</span></a></li>
</ul></nav><div class="header-search"><button class="mobile-search-toggle"><svg class="svg-icon search-icon" width="28" height="28" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M508.5 468.9L387.1 347.5c-2.3-2.3-5.3-3.5-8.5-3.5h-13.2c31.5-36.5 50.6-84 50.6-136C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c52 0 99.5-19.1 136-50.6v13.2c0 3.2 1.3 6.2 3.5 8.5l121.4 121.4c4.7 4.7 12.3 4.7 17 0l22.6-22.6c4.7-4.7 4.7-12.3 0-17zM208 368c-88.4 0-160-71.6-160-160S119.6 48 208 48s160 71.6 160 160-71.6 160-160 160z"/></svg><svg class="svg-icon search-close" width="28" height="28" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M207.6 256l107.72-107.72c6.23-6.23 6.23-16.34 0-22.58l-25.03-25.03c-6.23-6.23-16.34-6.23-22.58 0L160 208.4 52.28 100.68c-6.23-6.23-16.34-6.23-22.58 0L4.68 125.7c-6.23 6.23-6.23 16.34 0 22.58L112.4 256 4.68 363.72c-6.23 6.23-6.23 16.34 0 22.58l25.03 25.03c6.23 6.23 16.34 6.23 22.58 0L160 303.6l107.72 107.72c6.23 6.23 16.34 6.23 22.58 0l25.03-25.03c6.23-6.23 6.23-16.34 0-22.58L207.6 256z"/></svg><span class="screen-reader-text">Search</span></button>
<form role="search" method="get" class="search-form" action="https://www.pyimagesearch.com/">
	<label>
		<span class="screen-reader-text">Search...</span>
		<input type="search" class="search-field" placeholder="Search articles..." value="" name="s" title="Search for" />
	</label>
	<button type="submit" class="search-submit"><svg class="svg-icon search" width="20" height="20" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M508.5 468.9L387.1 347.5c-2.3-2.3-5.3-3.5-8.5-3.5h-13.2c31.5-36.5 50.6-84 50.6-136C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c52 0 99.5-19.1 136-50.6v13.2c0 3.2 1.3 6.2 3.5 8.5l121.4 121.4c4.7 4.7 12.3 4.7 17 0l22.6-22.6c4.7-4.7 4.7-12.3 0-17zM208 368c-88.4 0-160-71.6-160-160S119.6 48 208 48s160 71.6 160 160-71.6 160-160 160z"/></svg><span class="screen-reader-text">Submit</span></button>
</form>
</div><nav class="nav-mobile"><button class="mobile-menu-toggle"><span class="mobile-menu-open"><svg class="svg-icon menu-open" width="13" height="13" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"/></svg>Menu</span><span class="mobile-menu-close"><svg class="svg-icon menu-close" width="13" height="13" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M207.6 256l107.72-107.72c6.23-6.23 6.23-16.34 0-22.58l-25.03-25.03c-6.23-6.23-16.34-6.23-22.58 0L160 208.4 52.28 100.68c-6.23-6.23-16.34-6.23-22.58 0L4.68 125.7c-6.23 6.23-6.23 16.34 0 22.58L112.4 256 4.68 363.72c-6.23 6.23-6.23 16.34 0 22.58l25.03 25.03c6.23 6.23 16.34 6.23 22.58 0L160 303.6l107.72 107.72c6.23 6.23 16.34 6.23 22.58 0l25.03-25.03c6.23-6.23 6.23-16.34 0-22.58L207.6 256z"/></svg>Close</span><span class="screen-reader-text">Menu</span></button></nav></div></div></header><div class="pyi-page-hero"><div class="wrap"><p class="entry-meta"><span class="entry-categories"><a href="https://www.pyimagesearch.com/category/dlib/" rel="category tag">dlib</a> <a href="https://www.pyimagesearch.com/category/faces/" rel="category tag">Face Applications</a> <a href="https://www.pyimagesearch.com/category/facial-landmarks/" rel="category tag">Facial Landmarks</a> <a href="https://www.pyimagesearch.com/category/shape-predictors/" rel="category tag">Shape Predictors</a> <a href="https://www.pyimagesearch.com/category/tutorials/" rel="category tag">Tutorials</a></span></p><header class="entry-header"><h1 class="entry-title">Training a custom dlib shape predictor</h1>
</header><p class="entry-meta">by <span class="entry-author"><a href="https://www.pyimagesearch.com/author/adrian/" class="entry-author-link" rel="author"><span class="entry-author-name">Adrian Rosebrock</span></a></span> on <time class="entry-time">December 16, 2019</time></p><div class="pyi-hero-left"></div><div class="pyi-hero-right"></div></div></div><div class="site-inner"><div class="wrap"><div class="content-sidebar-wrap"><main class="content" id="genesis-content"><article class="post-11908 post type-post status-publish format-standard has-post-thumbnail category-dlib category-faces category-facial-landmarks category-shape-predictors category-tutorials tag-dlib tag-facial-landmarks tag-landmark-predictors tag-shape-predictors entry"><div class="entry-content">
<div class="single-post-sticky-spacer">
	<div id="source-code-mini-wrap" class="gpd-source-code-mini single-post-top-cta" style="">
   		<div class="gpd-source-code-mini-content">
			<a href="#download-the-code" id="pyis-optinmonster-open-modal">
			Click here to download the source code to this post			</a>
    	</div>
	</div>
</div><p><script src="https://fast.wistia.com/embed/medias/ptn7l1ysjj.jsonp" async=""></script><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script></p>
<div class="wistia_responsive_padding" style="padding: 61.88% 0 0 0; position: relative;">
<div class="wistia_responsive_wrapper" style="height: 100%; left: 0; position: absolute; top: 0; width: 100%;">
<div class="wistia_embed wistia_async_ptn7l1ysjj videoFoam=true" style="height: 100%; position: relative; width: 100%;"></div>
</div>
</div>
<p>In this tutorial, you will learn how to train your own custom dlib shape predictor. You&#8217;ll then learn how to take your trained dlib shape predictor and use it to predict landmarks on input images and real-time video streams.</p>
<p>Today kicks off a brand new two-part series on <strong>training custom shape predictors with dlib:</strong></p>
<ol>
<li><strong>Part #1:</strong> Training a custom dlib shape predictor (today&#8217;s tutorial)</li>
<li><strong>Part #2:</strong> Tuning dlib shape predictor hyperparameters to balance speed, accuracy, and model size (next week&#8217;s tutorial)</li>
</ol>
<p><strong>Shape predictors</strong>, also called <strong>landmark predictors</strong>, are used to predict key <em>(x, y)</em>-coordinates of a given &#8220;shape&#8221;.</p>
<p>The most common, well-known shape predictor is <a href="https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/" target="_blank" rel="noopener noreferrer"><strong>dlib&#8217;s facial landmark predictor</strong></a> used to localize individual facial structures, including the:</p>
<ul>
<li>Eyes</li>
<li>Eyebrows</li>
<li>Nose</li>
<li>Lips/mouth</li>
<li>Jawline</li>
</ul>
<p>Facial landmarks are used for <a href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/" target="_blank" rel="noopener noreferrer">face alignment</a> (a method to improve face recognition accuracy), building a <a href="https://pyimagesearch.com/2017/05/08/drowsiness-detection-opencv/" target="_blank" rel="noopener noreferrer">&#8220;drowsiness detector&#8221; to detect tired, sleepy drivers behind the wheel</a>, face swapping, virtual makeover applications, <em>and much more.</em></p>
<p><strong>However, just because facial landmarks are the most popular type of shape predictor, doesn&#8217;t mean we can&#8217;t train a shape predictor to localize <em>other shapes</em> in an image!</strong></p>
<p>For example, you could use a shape predictor to:</p>
<ul>
<li>Automatically localize the four corners of a piece of paper when building a <a href="https://pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/" target="_blank" rel="noopener noreferrer">computer vision-based document scanner</a>.</li>
<li>Detect the key, structural joints of the human body (feet, knees, elbows, etc.).</li>
<li>Localize the tips of your fingers when building an AR/VR application.</li>
</ul>
<p>Today we&#8217;ll be exploring shape predictors in more detail, including how you can train your own custom shape predictor using the dlib library.</p>
<p><strong>To learn how to train your own dlib shape predictor, <em>just keep reading!</em></strong></p>
<div id="pyi-source-code-block" class="source-code-wrap"><div class="gpd-source-code">
    <div class="gpd-source-code-content">
        <img src="//www.pyimagesearch.com/wp-content/uploads/2020/01/source-code-icon.png" alt="">
        <h4>Looking for the source code to this post?</h4>
                    <a href="#download-the-code">Jump Right To The Downloads Section <svg class="svg-icon arrow-right" width="12" height="12" aria-hidden="true" role="img" focusable="false" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.8125 0.1875C6.875 0.125 6.96875 0.09375 7.09375 0.09375C7.1875 0.09375 7.28125 0.125 7.34375 0.1875L13.875 6.75C13.9375 6.8125 14 6.90625 14 7C14 7.125 13.9375 7.1875 13.875 7.25L7.34375 13.8125C7.28125 13.875 7.1875 13.9062 7.09375 13.9062C6.96875 13.9062 6.875 13.875 6.8125 13.8125L6.1875 13.1875C6.125 13.125 6.09375 13.0625 6.09375 12.9375C6.09375 12.8438 6.125 12.75 6.1875 12.6562L11.0312 7.8125H0.375C0.25 7.8125 0.15625 7.78125 0.09375 7.71875C0.03125 7.65625 0 7.5625 0 7.4375V6.5625C0 6.46875 0.03125 6.375 0.09375 6.3125C0.15625 6.25 0.25 6.1875 0.375 6.1875H11.0312L6.1875 1.34375C6.125 1.28125 6.09375 1.1875 6.09375 1.0625C6.09375 0.96875 6.125 0.875 6.1875 0.8125L6.8125 0.1875Z" fill="#169FE6"></path></svg></a>
            </div>
</div>
</div>
<h2>Tuning a custom dlib shape predictor</h2>
<p>In the first part of this tutorial, we&#8217;ll briefly discuss what shape/landmark predictors are and how they can be used to predict specific locations on structural objects.</p>
<p>From there we&#8217;ll review the iBUG 300-W dataset, a common dataset used to train shape predictors used to localize specific locations on the human face (i.e., <strong>facial landmarks</strong>).</p>
<p>I&#8217;ll then show you how to train your own custom dlib shape predictor, resulting in a model that can balance speed, accuracy, and model size.</p>
<p>Finally, we&#8217;ll put our shape predictor to the test and apply it to a set of input images/video streams, demonstrating that our shape predictor is capable of running in real-time.</p>
<p>We&#8217;ll wrap up the tutorial with a discussion of next steps.</p>
<h3>What are shape/landmark predictors?</h3>
<figure id="attachment_11915" aria-describedby="caption-attachment-11915" style="width: 600px" class="wp-caption aligncenter"><a href="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_example_pred.jpg"><img class="wp-image-11915 size-full" src="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_example_pred.jpg" alt="" width="600" height="445" srcset="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_example_pred.jpg 600w, https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_example_pred-300x223.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a><figcaption id="caption-attachment-11915" class="wp-caption-text"><strong>Figure 1:</strong> Training a custom dlib shape predictor on facial landmarks (<a href="http://www.csc.kth.se/~vahidk/face_ert.html" target="_blank" rel="noopener noreferrer">image source</a>).</figcaption></figure>
<p>Shape/landmark predictors are used to localize specific <em>(x, y)</em>-coordinates on an input &#8220;shape&#8221;. The term &#8220;shape&#8221; is arbitrary, <strong>but it&#8217;s assumed that the shape is <em>structural in nature.</em></strong></p>
<p>Examples of structural shapes include:</p>
<ul>
<li>Faces</li>
<li>Hands</li>
<li>Fingers</li>
<li>Toes</li>
<li>etc.</li>
</ul>
<p>For example, faces come in all different shapes and sizes, and they <em>all</em> share common structural characteristics &#8212; the eyes are above the nose, the nose is above the mouth, etc.</p>
<p><strong>The goal of shape/landmark predictors is to exploit this structural knowledge and given enough training data, <em>learn how to automatically predict the location of these structures.</em></strong></p>
<h3>How do shape/landmark predictors work?</h3>
<figure id="attachment_11916" aria-describedby="caption-attachment-11916" style="width: 500px" class="wp-caption aligncenter"><a href="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ert.png"><img class="wp-image-11916" src="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ert.png" alt="" width="500" height="356" srcset="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ert.png 372w, https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ert-300x214.png 300w" sizes="(max-width: 500px) 100vw, 500px" /></a><figcaption id="caption-attachment-11916" class="wp-caption-text"><strong>Figure 2:</strong> How do shape/landmark predictors work? The dlib library implements a shape predictor algorithm with an ensemble of regression trees approach using the method described by <a href="http://www.csc.kth.se/~vahidk/face_ert.html" target="_blank" rel="noopener noreferrer">Kazemi and Sullivan in their 2014 CVPR paper</a> (<a href="https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html" target="_blank" rel="noopener noreferrer">image source</a>).</figcaption></figure>
<p>There are a variety of shape predictor algorithms. Exactly which one you use depends on whether:</p>
<ul>
<li>You&#8217;re working with 2D or 3D data</li>
<li>You need to utilize deep learning</li>
<li>Or, if traditional Computer Vision and Machine Learning algorithms will suffice</li>
</ul>
<p>The shape predictor algorithm implemented in the dlib library comes from Kazemi and Sullivan&#8217;s 2014 CVPR paper, <em><a href="http://www.csc.kth.se/~vahidk/face_ert.html" target="_blank" rel="noopener noreferrer">One Millisecond Face Alignment with an Ensemble of Regression Trees.</a></em></p>
<p>To estimate the landmark locations, the algorithm:</p>
<ul>
<li>Examines a sparse set of input pixel intensities (i.e., the &#8220;features&#8221; to the input model)</li>
<li>Passes the features into an Ensemble of Regression Trees (ERT)</li>
<li>Refines the predicted locations to improve accuracy through a cascade of regressors</li>
</ul>
<p><strong>The end result is a shape predictor that can run in super real-time!</strong></p>
<p>For more details on the inner-workings of the landmark prediction, <a href="http://www.csc.kth.se/~vahidk/face_ert.html" target="_blank" rel="noopener noreferrer">be sure to refer to Kazemi and Sullivan&#8217;s 2014 publication.</a></p>
<h3>The iBUG 300-W dataset</h3>
<figure id="attachment_11918" aria-describedby="caption-attachment-11918" style="width: 500px" class="wp-caption aligncenter"><a href="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ibug_group.png"><img class="wp-image-11918" src="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ibug_group.png" alt="" width="500" height="196" srcset="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ibug_group.png 373w, https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_ibug_group-300x117.png 300w" sizes="(max-width: 500px) 100vw, 500px" /></a><figcaption id="caption-attachment-11918" class="wp-caption-text"><strong>Figure 3:</strong> In this tutorial we will use the iBUG 300-W face landmark dataset to learn how to train a custom dlib shape predictor.</figcaption></figure>
<p>To train our custom dlib shape predictor, we&#8217;ll be utilizing the <a href="https://ibug.doc.ic.ac.uk/resources/300-W/" target="_blank" rel="noopener noreferrer">iBUG 300-W dataset</a> (but with a twist).</p>
<p>The goal of iBUG-300W is to train a shape predictor capable of localizing each individual facial structure, including the eyes, eyebrows, nose, mouth, and jawline.</p>
<p>The dataset itself consists of 68 pairs of integer values &#8212; <strong>these values are the <em>(x, y)</em>-coordinates of the facial structures depicted in Figure 2 above.</strong></p>
<p>To create the iBUG-300W dataset, researchers manually and painstakingly annotated and labeled <em>each</em> of the 68 coordinates on a total of 7,764 images.</p>
<p>A model trained on iBUG-300W can predict the location of each of these 68 <em>(x, y)</em>-coordinate pairs and can, therefore, localize each of the locations on the face.</p>
<p>That&#8217;s all fine and good&#8230;</p>
<p><strong>&#8230;but what if we wanted to train a shape predictor to localize <em>just</em> the eyes?</strong></p>
<p>How might we go about doing that?</p>
<h3>Balancing shape predictor model speed and accuracy</h3>
<figure id="attachment_11919" aria-describedby="caption-attachment-11919" style="width: 600px" class="wp-caption aligncenter"><a href="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_landmark_comparison.jpg"><img class="size-full wp-image-11919" src="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_landmark_comparison.jpg" alt="" width="600" height="821" srcset="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_landmark_comparison.jpg 600w, https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_landmark_comparison-219x300.jpg 219w" sizes="(max-width: 600px) 100vw, 600px" /></a><figcaption id="caption-attachment-11919" class="wp-caption-text"><strong>Figure 4:</strong> We will train a custom dlib shape/landmark predictor to recognize just eyes in this tutorial.</figcaption></figure>
<p><strong>Let&#8217;s suppose for a second that you want to train a custom shape predictor to localize <em>just</em> the location of the eyes.</strong></p>
<p>We would have two options to accomplish this task:</p>
<ol>
<li>Utilize dlib&#8217;s <em>pre-trained</em> facial landmark detector used to localize <em>all</em> facial structures and then discard all localizations <em>except</em> for the eyes.</li>
<li>Train our own <em>custom</em> dlib landmark predictor that returns <em>just</em> the locations of the eyes.</li>
</ol>
<p>In some cases you may be able to get away with the first option; however, there are two problems there, namely regarding your <strong>model speed</strong> and your <strong>model size.</strong></p>
<p><strong>Model speed:</strong> Even though you&#8217;re only interested in a <em>subset</em> of the landmark predictions, your model is <em>still responsible</em> for predicting the <em>entire set</em> of landmarks. You can&#8217;t just tell your model <em>&#8220;Oh hey, just give me those locations, don&#8217;t bother computing the rest.&#8221;</em> It doesn&#8217;t work like that &#8212; it&#8217;s an &#8220;all or nothing&#8221; calculation.</p>
<p><strong>Model size:</strong> Since your model needs to know how to predict <em>all</em> landmark locations it was trained on, it therefore needs to store quantified information on how to predict each of these locations. The more information it needs to store, the larger your model size is.</p>
<p><strong>Think of your shape predictor model size as a grocery list</strong> &#8212; out of a list of 20 items, you may only truly <em>need</em> eggs and a gallon of milk, but if you&#8217;re heading to the store, you&#8217;re going to be purchasing <em>all</em> the items on that list because that&#8217;s what your family expects you to do!</p>
<p>The model size is the same way.</p>
<p>Your model doesn&#8217;t &#8220;care&#8221; that you only truly &#8220;need&#8221; a subset of the landmark predictions; it was trained to predict all of them so you&#8217;re going to get all of them in return!</p>
<p><strong>If you only need a <em>subset</em> of specific landmarks you should consider training your own custom shape predictor</strong> &#8212; you&#8217;ll end up with a model that is both <em>smaller</em> and <em>faster.</em></p>
<p><strong>In the context of today&#8217;s tutorial, we&#8217;ll be training a custom dlib shape predictor to localize <em>just the eye locations</em> from the iBUG 300-W dataset.</strong></p>
<p>Such a model could be utilized in a virtual makeover application used to apply <em>just</em> eyeliner/mascara <em>or</em> it could be used in a <a href="https://pyimagesearch.com/2017/10/23/raspberry-pi-facial-landmarks-drowsiness-detection-with-opencv-and-dlib/" target="_blank" rel="noopener noreferrer">drowsiness detector used to detect tired drivers behind the wheel of a car</a>.</p>
<h3>Configuring your dlib development environment</h3>
<p>To follow along with today&#8217;s tutorial, you will need a virtual environment with the following packages installed:</p>
<ul>
<li>dlib</li>
<li>OpenCV</li>
<li>imutils</li>
</ul>
<p>Luckily, each of these packages is pip-installable, but there are a handful of pre-requisites including virtual environments. Be sure to follow these two guides for additional information:</p>
<ul>
<li><strong><a href="https://pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/" target="_blank" rel="noopener noreferrer"><em>Install dlib (the easy, complete guide)</em></a></strong></li>
<li><strong><a href="https://pyimagesearch.com/2018/09/19/pip-install-opencv/" target="_blank" rel="noopener noreferrer"><em>pip install opencv</em></a></strong></li>
</ul>
<p>The pip install commands include:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="0">$ workon &lt;env-name>
$ pip install dlib
$ pip install opencv-contrib-python
$ pip install imutils
</pre>


<p>The <code class="EnlighterJSRAW" data-enlighter-language="shell">workon</code>&nbsp; command becomes available once you install <code class="EnlighterJSRAW" data-enlighter-language="python">virtualenv</code>&nbsp; and <code class="EnlighterJSRAW" data-enlighter-language="python">virtualenvwrapper</code>&nbsp; per either my <a href="https://pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/" target="_blank" rel="noopener noreferrer">dlib</a> or <a href="https://pyimagesearch.com/2018/09/19/pip-install-opencv/" target="_blank" rel="noopener noreferrer">OpenCV</a> installation guides.</p>
<h3>Downloading the iBUG 300-W dataset</h3>
<p>Before we get too far into this tutorial, <strong>take a second now to download the iBUG 300-W dataset (~1.7GB):</strong></p>
<p style="padding-left: 40px;"><a href="http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz" target="_blank" rel="noopener noreferrer">http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz</a></p>
<p>You&#8217;ll also want to use the <strong><em>&#8220;Downloads&#8221;</em></strong> section of this blog post to download the source code.</p>
<p>I recommend placing the iBug 300W dataset into the zip associated with the download of this tutorial like this:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="4">$ unzip custom-dlib-shape-predictor.zip
...
$ cd custom-dlib-shape-predictor
$ mv ~/Downloads/ibug_300W_large_face_landmark_dataset.tar.gz .
$ tar -xvf ibug_300W_large_face_landmark_dataset.tar.gz
...
</pre>


<p>Alternatively (i.e. rather than clicking the hyperlink above), use <code class="EnlighterJSRAW" data-enlighter-language="shell">wget</code>&nbsp; in your terminal to download the dataset directly:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="6">$ unzip custom-dlib-shape-predictor.zip
...
$ cd custom-dlib-shape-predictor
$ wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz
$ tar -xvf ibug_300W_large_face_landmark_dataset.tar.gz
...
</pre>


<p>From there you can follow along with the rest of the tutorial.</p>
<h3>Project Structure</h3>
<p>Assuming you have followed the instructions in the previous section, your project directory is now organized as follows:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="7">$ tree --dirsfirst --filelimit 10
.
├── ibug_300W_large_face_landmark_dataset
│   ├── afw [1011 entries]
│   ├── helen
│   │   ├── testset [990 entries]
│   │   └── trainset [6000 entries]
│   ├── ibug [405 entries]
│   ├── image_metadata_stylesheet.xsl
│   ├── labels_ibug_300W.xml
│   ├── labels_ibug_300W_test.xml
│   ├── labels_ibug_300W_train.xml
│   └── lfpw
│       ├── testset [672 entries]
│       └── trainset [2433 entries]
├── ibug_300W_large_face_landmark_dataset.tar.gz
├── eye_predictor.dat
├── parse_xml.py
├── train_shape_predictor.py
├── evaluate_shape_predictor.py
└── predict_eyes.py

9 directories, 10 files
</pre>


<p>The iBug 300-W dataset is extracted in the <code class="EnlighterJSRAW" data-enlighter-language="python">ibug_300W_large_face_landmark_dataset/</code>&nbsp; directory. We will review the following Python scripts in this order:</p>
<ol>
<li><code class="EnlighterJSRAW" data-enlighter-language="shell">parse_xml.py</code> : Parses the train/test XML dataset files for <strong>eyes-only</strong> landmark coordinates.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="shell">train_shape_predictor.py</code>&nbsp;: Accepts the parsed XML files to train our shape predictor with dlib.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="shell">evaluate_shape_predictor.py</code> : Calculates the Mean Average Error (MAE) of our custom shape predictor.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="shell">predict_eyes.py</code> : Performs shape prediction using our custom dlib shape predictor, trained to only recognize eye landmarks.</li>
</ol>
<p>We&#8217;ll begin by inspecting our input XML files in the next section.</p>
<h3>Understanding the iBUG-300W XML file structure</h3>
<p>We&#8217;ll be using the iBUG-300W to train our shape predictor; however, we have a bit of a problem:</p>
<p>iBUG-300W supplies <em>(x, y)</em>-coordinate pairs for <strong><em>all</em></strong> facial structures in the dataset (i.e., eyebrows, eyes, nose, mouth, and jawline)&#8230;</p>
<p><strong>&#8230;however, we want to train our shape predictor on <em>just the eyes!</em></strong></p>
<p>So, what are we going to do?</p>
<p>Are we going to find another dataset that doesn&#8217;t include the facial structures we don&#8217;t care about?</p>
<p>Manually open up the training file and delete the coordinate pairs for the facial structures we don&#8217;t need?</p>
<p>Simply give up, take our ball, and go home?</p>
<p><strong>Of course not!</strong></p>
<p>We&#8217;re programmers and engineers &#8212; all we need is some basic file parsing to create a new training file that includes <em>just</em> the eye coordinates.</p>
<p>To understand how we can do that, let&#8217;s first consider how facial landmarks are annotated in the iBUG-300W dataset by examining the <code class="EnlighterJSRAW" data-enlighter-language="python">labels_ibug_300W_train.xml</code> training file:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="25" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="14">...
&lt;images>
  &lt;image file='lfpw/trainset/image_0457.png'>
    &lt;box top='78' left='74' width='138' height='140'>
      &lt;part name='00' x='55' y='141'/>
      &lt;part name='01' x='59' y='161'/>
      &lt;part name='02' x='66' y='182'/>
      &lt;part name='03' x='75' y='197'/>
      &lt;part name='04' x='90' y='209'/>
      &lt;part name='05' x='108' y='220'/>
      &lt;part name='06' x='131' y='226'/>
      &lt;part name='07' x='149' y='232'/>
      &lt;part name='08' x='167' y='230'/>
      &lt;part name='09' x='181' y='225'/>
      &lt;part name='10' x='184' y='208'/>
      &lt;part name='11' x='186' y='193'/>
      &lt;part name='12' x='185' y='179'/>
      &lt;part name='13' x='184' y='167'/>
      &lt;part name='14' x='186' y='152'/>
      &lt;part name='15' x='185' y='142'/>
      &lt;part name='16' x='181' y='133'/>
      &lt;part name='17' x='95' y='128'/>
      &lt;part name='18' x='105' y='121'/>
      &lt;part name='19' x='117' y='117'/>
      &lt;part name='20' x='128' y='115'/>
      &lt;part name='21' x='141' y='116'/>
      &lt;part name='22' x='156' y='115'/>
      &lt;part name='23' x='162' y='110'/>
      &lt;part name='24' x='169' y='108'/>
      &lt;part name='25' x='175' y='108'/>
      &lt;part name='26' x='180' y='109'/>
      &lt;part name='27' x='152' y='127'/>
      &lt;part name='28' x='157' y='136'/>
      &lt;part name='29' x='162' y='145'/>
      &lt;part name='30' x='168' y='154'/>
      &lt;part name='31' x='152' y='166'/>
      &lt;part name='32' x='158' y='166'/>
      &lt;part name='33' x='163' y='168'/>
      &lt;part name='34' x='167' y='166'/>
      &lt;part name='35' x='171' y='164'/>
      &lt;part name='36' x='111' y='134'/>
      &lt;part name='37' x='116' y='130'/>
      &lt;part name='38' x='124' y='128'/>
      &lt;part name='39' x='129' y='130'/>
      &lt;part name='40' x='125' y='134'/>
      &lt;part name='41' x='118' y='136'/>
      &lt;part name='42' x='161' y='127'/>
      &lt;part name='43' x='166' y='123'/>
      &lt;part name='44' x='173' y='122'/>
      &lt;part name='45' x='176' y='125'/>
      &lt;part name='46' x='173' y='129'/>
      &lt;part name='47' x='167' y='129'/>
      &lt;part name='48' x='139' y='194'/>
      &lt;part name='49' x='151' y='186'/>
      &lt;part name='50' x='159' y='180'/>
      &lt;part name='51' x='163' y='182'/>
      &lt;part name='52' x='168' y='180'/>
      &lt;part name='53' x='173' y='183'/>
      &lt;part name='54' x='176' y='189'/>
      &lt;part name='55' x='174' y='193'/>
      &lt;part name='56' x='170' y='197'/>
      &lt;part name='57' x='165' y='199'/>
      &lt;part name='58' x='160' y='199'/>
      &lt;part name='59' x='152' y='198'/>
      &lt;part name='60' x='143' y='194'/>
      &lt;part name='61' x='159' y='186'/>
      &lt;part name='62' x='163' y='187'/>
      &lt;part name='63' x='168' y='186'/>
      &lt;part name='64' x='174' y='189'/>
      &lt;part name='65' x='168' y='191'/>
      &lt;part name='66' x='164' y='192'/>
      &lt;part name='67' x='160' y='192'/>
    &lt;/box>
  &lt;/image>
...
</pre>


<p>All training data in the iBUG-300W dataset is represented by a structured XML file.</p>
<p>Each image has an <code class="EnlighterJSRAW" data-enlighter-language="python">image</code> tag.</p>
<p>Inside the <code class="EnlighterJSRAW" data-enlighter-language="python">image</code> tag is a <code class="EnlighterJSRAW" data-enlighter-language="python">file</code> attribute that points to where the example image file resides on disk.</p>
<p>Additionally, each <code class="EnlighterJSRAW" data-enlighter-language="python">image</code> has a <code class="EnlighterJSRAW" data-enlighter-language="python">box</code> element associated with it.</p>
<p>The <code class="EnlighterJSRAW" data-enlighter-language="python">box</code> element represents the <strong>bounding box coordinates of the face</strong> in the image. To understand how the <code class="EnlighterJSRAW" data-enlighter-language="python">box</code> element represents the bounding box of the face, consider its four attributes:</p>
<ol>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">top</code>: The starting <em>y</em>-coordinate of the bounding box.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">left</code>: The starting <em>x</em>-coordinate of the bounding box.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">width</code>: The width of the bounding box.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">height</code>: The height of the bounding box.</li>
</ol>
<p>Inside the <code class="EnlighterJSRAW" data-enlighter-language="python">box</code> element we have a total of 68 <code class="EnlighterJSRAW" data-enlighter-language="python">part</code> elements &#8212; <strong>these <code class="EnlighterJSRAW" data-enlighter-language="python">part</code> elements represent the individual <em>(x, y)</em>-coordinates of the facial landmarks in the iBUG-300W dataset.</strong></p>
<p>Notice that each <code class="EnlighterJSRAW" data-enlighter-language="python">part</code> element has three attributes:</p>
<ol>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">name</code>: The index/name of the specific facial landmark.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">x</code>: The <em>x</em>-coordinate of the landmark.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">y</code>: The <em>y</em>-coordinate of the landmark.</li>
</ol>
<p><strong>So, how do these landmarks map to specific facial structures?</strong></p>
<p>The answer lies in the following figure:</p>
<figure id="attachment_5309" aria-describedby="caption-attachment-5309" style="width: 600px" class="wp-caption aligncenter"><a href="https://pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg"><img class="wp-image-5309" src="https://pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg" alt="" width="600" height="484" srcset="https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg 1856w, https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup-300x242.jpg 300w, https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup-768x619.jpg 768w, https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup-1024x825.jpg 1024w" sizes="(max-width: 600px) 100vw, 600px" /></a><figcaption id="caption-attachment-5309" class="wp-caption-text"><strong>Figure 5:</strong> Visualizing the 68 facial landmark coordinates from the iBUG 300-W dataset.</figcaption></figure>
<p>The coordinates in <strong><strong>Figure 5</strong></strong> are 1-indexed so to map the coordinate <code class="EnlighterJSRAW" data-enlighter-language="python">name</code> to our XML file, simply subtract 1 from the value (since our XML file is 0-indexed).</p>
<p>Based on the visualization, we can then derive which <code class="EnlighterJSRAW" data-enlighter-language="python">name</code> coordinates maps to which facial structure:</p>
<ul>
<li>The&nbsp;<strong>mouth</strong>&nbsp;can be accessed through points&nbsp;<em>[48, 68]</em>.</li>
<li>The&nbsp;<strong>right eyebrow</strong>&nbsp;through points&nbsp;<em>[17, 22].</em></li>
<li>The&nbsp;<strong>left eyebrow</strong>&nbsp;through points&nbsp;<em>[22, 27]</em>.</li>
<li>The&nbsp;<strong>right eye</strong>&nbsp;using&nbsp;<em>[36, 42].</em></li>
<li>The&nbsp;<strong>left eye</strong>&nbsp;with&nbsp;<em>[42, 48].</em></li>
<li>The&nbsp;<strong>nose</strong>&nbsp;using&nbsp;<em>[27, 35].</em></li>
<li>And the <strong>jaw</strong> via&nbsp;<em>[0, 17].</em></li>
</ul>
<p>Since we&#8217;re <em>only interested in the eyes</em>, we therefore need to parse out points <em>[36, 48)</em>, again keeping in mind that:</p>
<ul>
<li>Our coordinates are zero-indexed in the XML file</li>
<li>And the closing parenthesis <em>&#8220;)&#8221;</em> in <em>[36, 48)</em>&nbsp;is mathematical notation implying &#8220;non-inclusive&#8221;.</li>
</ul>
<p>Now that we understand the structure of the iBUG-300W training file, we can move on to parsing out <em>only</em> the eye coordinates.</p>
<h3>Building an &#8220;eyes only&#8221; shape predictor dataset</h3>
<p>Let&#8217;s create a Python script to parse the iBUG-300W XML files and extract <em>only</em> the eye coordinates (which we&#8217;ll then train a custom dlib shape predictor on in the following section).</p>
<p>Open up the <code class="EnlighterJSRAW" data-enlighter-language="shell">parse_xml.py</code> file and we&#8217;ll get started:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="36"># import the necessary packages
import argparse
import re

# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--input", required=True,
	help="path to iBug 300-W data split XML file")
ap.add_argument("-t", "--output", required=True,
	help="path output data split XML file")
args = vars(ap.parse_args())
</pre>


<p><strong>Lines 2 and 3</strong> import necessary packages.</p>
<p>We&#8217;ll use two of Python&#8217;s built-in modules: (1) <code class="EnlighterJSRAW" data-enlighter-language="python">argparse</code>&nbsp; for <a href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/" target="_blank" rel="noopener noreferrer">parsing command line arguments</a>, and (2) <code class="EnlighterJSRAW" data-enlighter-language="python">re</code>&nbsp;&nbsp;for regular expression matching. If you ever need help developing regular expressions, <a href="https://regex101.com/" target="_blank" rel="noopener noreferrer">regex101.com</a> is a great tool and supports languages other than Python as well.</p>
<p>Our script requires two command line arguments:</p>
<ul>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">--input</code> : The path to our input data split XML file (i.e. from the iBug 300-W dataset).</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">--output</code> : The path to our output <strong>eyes-only</strong>&nbsp;XML file.</li>
</ul>
<p>Let&#8217;s go ahead and define the indices of our eye coordinates:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="13" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="41"># in the iBUG 300-W dataset, each (x, y)-coordinate maps to a specific
# facial feature (i.e., eye, mouth, nose, etc.) -- in order to train a
# dlib shape predictor on *just* the eyes, we must first define the
# integer indexes that belong to the eyes
LANDMARKS = set(list(range(36, 48)))
</pre>


<p>Our <strong>eye landmarks</strong> are specified on&nbsp;<strong>Line 17</strong>. Refer to&nbsp;<strong>Figure 5</strong>, keeping in mind that the figure is 1-indexed while Python is 0-indexed.</p>
<p>We&#8217;ll be training our custom shape predictor on eye locations; however, <strong>you could just as easily train an eyebrow, nose, mouth, or jawline predictor, including any combination or subset of these structures,</strong> by modifying the <code class="EnlighterJSRAW" data-enlighter-language="python">LANDMARKS</code> list and including the 0-indexed names of the landmarks you want to detect.</p>
<p>Now let&#8217;s define our regular expression and load the original input XML file:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="19" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="43"># to easily parse out the eye locations from the XML file we can
# utilize regular expressions to determine if there is a 'part'
# element on any given line
PART = re.compile("part name='[0-9]+'")

# load the contents of the original XML file and open the output file
# for writing
print("[INFO] parsing data split XML file...")
rows = open(args["input"]).read().strip().split("\n")
output = open(args["output"], "w")
</pre>


<p>Our regular expression on <strong>Line 22</strong> will soon enable extracting <code class="EnlighterJSRAW" data-enlighter-language="python">part</code> elements along with their names/indexes.</p>
<p><strong>Line 27</strong> loads the contents of <em>input</em> XML file.</p>
<p><strong>Line 28</strong> opens our <em>output</em> XML file for writing.</p>
<p>Now we&#8217;re ready to loop over the&nbsp;<em>input</em> XML file to <strong>find and extract the eye landmarks:</strong></p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="30" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="45"># loop over the rows of the data split file
for row in rows:
	# check to see if the current line has the (x, y)-coordinates for
	# the facial landmarks we are interested in
	parts = re.findall(PART, row)

	# if there is no information related to the (x, y)-coordinates of
	# the facial landmarks, we can write the current line out to disk
	# with no further modifications
	if len(parts) == 0:
		output.write("{}\n".format(row))

	# otherwise, there is annotation information that we must process
	else:
		# parse out the name of the attribute from the row
		attr = "name='"
		i = row.find(attr)
		j = row.find("'", i + len(attr) + 1)
		name = int(row[i + len(attr):j])

		# if the facial landmark name exists within the range of our
		# indexes, write it to our output file
		if name in LANDMARKS:
			output.write("{}\n".format(row))

# close the output file
output.close()
</pre>


<p><strong>Line 31</strong> begins a loop over the <code class="EnlighterJSRAW" data-enlighter-language="python">rows</code>&nbsp; of the input XML file. Inside the loop, we perform the following tasks:</p>
<ul>
<li>Determine if the current <code class="EnlighterJSRAW" data-enlighter-language="python">row</code> contains a <code class="EnlighterJSRAW" data-enlighter-language="python">part</code> element via regular expression matching (<strong>Line 34</strong>).
<ul>
<li>If it <strong><em>does not</em> contain a <code class="EnlighterJSRAW" data-enlighter-language="python">part</code> element,</strong> write the row back out to file (<strong>Lines 39 and 40</strong>).</li>
<li>If it <strong><em>does contain a <code class="EnlighterJSRAW" data-enlighter-language="python">part</code> element</em></strong>, we need to parse it further (<strong>Lines 43-53</strong>).
<ul>
<li>Here we extract <code class="EnlighterJSRAW" data-enlighter-language="python">name</code> attribute from the <code class="EnlighterJSRAW" data-enlighter-language="python">part</code>.</li>
<li>And then check to see if the <code class="EnlighterJSRAW" data-enlighter-language="python">name</code> exists in the <code class="EnlighterJSRAW" data-enlighter-language="python">LANDMARKS</code> we want to train a shape predictor to localize. If so, we write the <code class="EnlighterJSRAW" data-enlighter-language="python">row</code> back out to disk (otherwise we <em>ignore</em> the particular <code class="EnlighterJSRAW" data-enlighter-language="python">name</code> as it&#8217;s not a landmark we want to localize).</li>
</ul>
</li>
</ul>
</li>
<li>Wrap up the script by closing our <code class="EnlighterJSRAW" data-enlighter-language="python">output</code> XML file (<strong>Line 56</strong>).</li>
</ul>
<p><em><strong>Note:</strong> Most of our <code class="EnlighterJSRAW" data-enlighter-language="shell">parse_xml.py</code> script was inspired by <a href="https://github.com/Luca96/dlib-minified-models/blob/master/face_landmarks/training_script.py#L32" target="_blank" rel="noopener noreferrer">Luca Anzalone&#8217;s <code>slice_xml</code> function from their GitHub repo</a>. A big thank you to Luca for putting together such a simple, concise script that is highly effective!</em></p>
<h3>Creating our training and testing splits</h3>
<figure id="attachment_11920" aria-describedby="caption-attachment-11920" style="width: 600px" class="wp-caption aligncenter"><a href="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_finder.png"><img class="wp-image-11920 size-full" src="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_finder.png" alt="" width="600" height="306" srcset="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_finder.png 600w, https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_finder-300x153.png 300w" sizes="(max-width: 600px) 100vw, 600px" /></a><figcaption id="caption-attachment-11920" class="wp-caption-text"><strong>Figure 6:</strong> Creating our &#8220;eye only&#8221; face landmark training/testing XML files for training a dlib custom shape predictor with Python.</figcaption></figure>
<p>At this point in the tutorial I assume you have both:</p>
<ol>
<li>Downloaded the iBUG-300W dataset from the <em>&#8220;Downloading the iBUG 300-W dataset&#8221;</em> section above</li>
<li>Used the <strong><em>&#8220;Downloads&#8221;</em></strong> section of this tutorial to download the source code.</li>
</ol>
<p>You can use the following command to generate our new <strong>training file</strong> by parsing only the eye landmark coordinates from the original training file:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="59">$ python parse_xml.py \
	--input ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml \
	--output ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml
[INFO] parsing data split XML file...
</pre>


<p>Similarly, you can do the same to create our new <strong>testing file:</strong></p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="60">$ python parse_xml.py \
	--input ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml \
	--output ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_eyes.xml
[INFO] parsing data split XML file...
</pre>


<p>To verify that our new training/testing files have been created, check your iBUG-300W root dataset directory for the <code class="EnlighterJSRAW" data-enlighter-language="python">labels_ibug_300W_train_eyes.xml</code> and <code class="EnlighterJSRAW" data-enlighter-language="python">labels_ibug_300W_test_eyes.xml</code> files:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="5,7" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="63">$ cd ibug_300W_large_face_landmark_dataset
$ ls -lh *.xml    
-rw-r--r--@ 1 adrian  staff    21M Aug 16  2014 labels_ibug_300W.xml
-rw-r--r--@ 1 adrian  staff   2.8M Aug 16  2014 labels_ibug_300W_test.xml
-rw-r--r--  1 adrian  staff   602K Dec 12 12:54 labels_ibug_300W_test_eyes.xml
-rw-r--r--@ 1 adrian  staff    18M Aug 16  2014 labels_ibug_300W_train.xml
-rw-r--r--  1 adrian  staff   3.9M Dec 12 12:54 labels_ibug_300W_train_eyes.xml
$ cd ..
</pre>


<p>Notice that our <code class="EnlighterJSRAW" data-enlighter-language="shell">*_eyes.xml</code>&nbsp; files are highlighted. Both of these files are <em>significantly smaller in filesize</em>&nbsp;than their original, non-parsed counterparts.</p>
<h3>Implementing our custom dlib shape predictor training script</h3>
<p>Our dlib shape predictor training script is loosely based on (1) <a href="http://dlib.net/train_shape_predictor.py.html" target="_blank" rel="noopener noreferrer">dlib&#8217;s official example</a> and (2) <a href="https://medium.com/datadriveninvestor/training-alternative-dlib-shape-predictor-models-using-python-d1d8f8bd9f5c" target="_blank" rel="noopener noreferrer">Luca Anzalone&#8217;s excellent 2018 article</a>.</p>
<p>My primary contributions here are to:</p>
<ul>
<li>Supply a <strong>complete end-to-end example</strong> of creating a custom dlib shape predictor, including:
<ul>
<li>Training the shape predictor on a training set</li>
<li>Evaluating the shape predictor on a testing set</li>
</ul>
</li>
<li>Use the shape predictor to make predictions on custom images/video streams.</li>
<li><strong>Provide additional commentary on the hyperparameters</strong> you should be tuning.</li>
<li>Demonstrate how to <strong>systematically tune your shape predictor hyperparameters</strong> to balance speed, model size, and accuracy <strong><em>(next week&#8217;s tutorial).</em></strong></li>
</ul>
<p>To learn how to train your own dlib shape predictor, open up the <code class="EnlighterJSRAW" data-enlighter-language="shell">train_shape_predictor.py</code> file in your project structure and insert the following code:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="66"># import the necessary packages
import multiprocessing
import argparse
import dlib

# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-t", "--training", required=True,
	help="path to input training XML file")
ap.add_argument("-m", "--model", required=True,
	help="path serialized dlib shape predictor model")
args = vars(ap.parse_args())
</pre>


<p><strong>Lines 2-4</strong> import our packages, namely dlib. The dlib toolkit is a package developed by <a href="https://pyimagesearch.com/2018/10/01/pyimageconf-2018-recap/" target="_blank" rel="noopener noreferrer">PyImageConf 2018</a> speaker, <a href="https://pyimagesearch.com/2017/03/13/an-interview-with-davis-king-creator-of-the-dlib-toolkit/" target="_blank" rel="noopener noreferrer">Davis King</a>. We will use dlib to train our shape predictor.</p>
<p>The multiprocessing library will be used to grab and set the number of threads/processes we will use for training our shape predictor.</p>
<p>Our script requires two command line arguments (<strong>Lines 7-12</strong>):</p>
<ul>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">--training</code>&nbsp;: The path to our input training XML file. We will use the <strong>eyes-only</strong> XML file generated by the previous two sections.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">--model</code>&nbsp;: The path to the serialized dlib shape predictor output file.</li>
</ul>
<p>From here we need to set options (i.e., hyperparameters) prior to training the shape predictor.</p>
<p>While the following code blocks could be condensed into just 11 lines of code, the comments in both the code and in this tutorial provide additional information to help you both (1) understand the key options, and (2) configure and tune the options/hyperparameters for optimal performance.</p>
<p>In the remaining code blocks in this section I&#8217;ll be discussing the <strong>7 most important hyperparameters you can tune/set</strong> when training your own custom dlib shape predictor. These values are:</p>
<ol>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">tree_depth</code></li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">nu</code></li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">cascade_depth</code></li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">feature_pool_size</code></li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">num_test_splits</code></li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_amount</code></li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_translation_jitter</code></li>
</ol>
<p>We&#8217;ll begin with grabbing the default dlib shape predictor options:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="14" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="76"># grab the default options for dlib's shape predictor
print("[INFO] setting shape predictor options...")
options = dlib.shape_predictor_training_options()
</pre>


<p>From there, we&#8217;ll configure the <code class="EnlighterJSRAW" data-enlighter-language="python">tree_depth</code>&nbsp;option:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="18" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="78"># define the depth of each regression tree -- there will be a total
# of 2^tree_depth leaves in each tree; small values of tree_depth
# will be *faster* but *less accurate* while larger values will
# generate trees that are *deeper*, *more accurate*, but will run
# *far slower* when making predictions
options.tree_depth = 4
</pre>


<p>Here we define the <code class="EnlighterJSRAW" data-enlighter-language="python">tree_depth</code>, which, as the name suggests, controls the depth of each regression tree in the Ensemble of Regression Trees (ERTs). There will be <code class="EnlighterJSRAW" data-enlighter-language="python">2^tree_depth</code> leaves in each tree &#8212; <strong>you must be careful to balance <em>depth</em> with <em>speed.</em></strong></p>
<p>Smaller values of <code class="EnlighterJSRAW" data-enlighter-language="python">tree_depth</code> will lead to <em>more shallow trees</em> that are <em>faster</em>, but potentially <em>less accurate. </em>Larger values of <code class="EnlighterJSRAW" data-enlighter-language="python">tree_depth</code> will create <em>deeper trees</em> that are <em>slower</em>, but potentially <em>more accurate.</em></p>
<p>Typical values for <code class="EnlighterJSRAW" data-enlighter-language="python">tree_depth</code> are in the range <em>[2, 8].</em></p>
<p>The next parameter we&#8217;re going to explore is <code class="EnlighterJSRAW" data-enlighter-language="python">nu</code>, a regularization parameter:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="25" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="85"># regularization parameter in the range [0, 1] that is used to help
# our model generalize -- values closer to 1 will make our model fit
# the training data better, but could cause overfitting; values closer
# to 0 will help our model generalize but will require us to have
# training data in the order of 1000s of data points
options.nu = 0.1
</pre>


<p>The <code class="EnlighterJSRAW" data-enlighter-language="python">nu</code> option is a floating-point value (in the range <em>[0, 1]</em>) used as a regularization parameter to help our model generalize.</p>
<p>Values closer to <code class="EnlighterJSRAW" data-enlighter-language="python">1</code> will make our model fit the training data closer, but could potentially lead to overfitting. Values closer to <code class="EnlighterJSRAW" data-enlighter-language="python"> 0</code> will help our model generalize; however, there is a caveat to the generalization power &#8212; <strong>the closer <code class="EnlighterJSRAW" data-enlighter-language="python">nu</code> is to <code class="EnlighterJSRAW" data-enlighter-language="python"> 0</code>, the <em>more training data you&#8217;ll need.</em></strong></p>
<p>Typically, for small values of <code class="EnlighterJSRAW" data-enlighter-language="python">nu</code> you&#8217;ll need 1000s of training examples.</p>
<p>Our next parameter is the <code class="EnlighterJSRAW" data-enlighter-language="python">cascade_depth</code>:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="32" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="93"># the number of cascades used to train the shape predictor -- this
# parameter has a *dramtic* impact on both the *accuracy* and *output
# size* of your model; the more cascades you have, the more accurate
# your model can potentially be, but also the *larger* the output size
options.cascade_depth = 15
</pre>


<p>A series of cascades is used to refine and tune the initial predictions from the ERTs &#8212; <strong>the <code class="EnlighterJSRAW" data-enlighter-language="python">cascade_depth</code> will have a dramatic impact on both the <em>accuracy</em> and the <em>output file size</em> of your model.</strong></p>
<p>The more cascades you allow for, the larger your model will become (but potentially more accurate). The fewer cascades you allow, the smaller your model will be (but could be less accurate).</p>
<p>The following figure from Kazemi and Sullivan&#8217;s paper demonstrates the impact that the <code class="EnlighterJSRAW" data-enlighter-language="python">cascade_depth</code> has on facial landmark alignment:</p>
<figure id="attachment_11921" aria-describedby="caption-attachment-11921" style="width: 600px" class="wp-caption aligncenter"><a href="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_cascade_depth.jpg"><img class="wp-image-11921 size-full" src="https://pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_cascade_depth.jpg" alt="" width="600" height="125" srcset="https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_cascade_depth.jpg 600w, https://www.pyimagesearch.com/wp-content/uploads/2019/12/dlib_shape_pred_cascade_depth-300x63.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a><figcaption id="caption-attachment-11921" class="wp-caption-text"><strong>Figure 7:</strong> The <code>cascade_depth</code> parameter has a significant impact on the accuracy of your custom dlib shape/landmark predictor model.</figcaption></figure>
<p>Clearly you can see that the deeper the cascade, the better the facial landmark alignment.</p>
<p>Typically you&#8217;ll want to explore <code class="EnlighterJSRAW" data-enlighter-language="python">cascade_depth</code> values in the range <em>[6, 18]</em>, depending on your required target model size and accuracy.</p>
<p>Let&#8217;s now move on to the <code class="EnlighterJSRAW" data-enlighter-language="python">feature_pool_size</code>:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="38" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="98"># number of pixels used to generate features for the random trees at
# each cascade -- larger pixel values will make your shape predictor
# more accurate, but slower; use large values if speed is not a
# problem, otherwise smaller values for resource constrained/embedded
# devices
options.feature_pool_size = 400
</pre>


<p>The <code class="EnlighterJSRAW" data-enlighter-language="python">feature_pool_size</code> controls the number of pixels used to generate features for the random trees in each cascade.</p>
<p>The <em>more pixels</em> you include, the slower your model will run (but could potentially be more accurate). The <em>fewer pixels</em> you take into account, the faster your model will run (but could also be less accurate).</p>
<p><strong>My recommendation here is that you should use large values for <code class="EnlighterJSRAW" data-enlighter-language="python">feature_pools_size</code> if inference speed is <em>not</em> a concern. Otherwise, you should use smaller values for faster prediction speed (typically for embedded/resource-constrained devices).</strong></p>
<p>The next parameter we&#8217;re going to set is the <code class="EnlighterJSRAW" data-enlighter-language="python">num_test_splits</code>:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="45" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="102"># selects best features at each cascade when training -- the larger
# this value is, the *longer* it will take to train but (potentially)
# the more *accurate* your model will be
options.num_test_splits = 50
</pre>


<p>The <code class="EnlighterJSRAW" data-enlighter-language="python">num_test_splits</code> parameter has a dramatic impact on <strong>how long it takes your model to train</strong> <em>(</em>i.e., training/wall clock time, <em>not</em> inference speed).</p>
<p>The more <code class="EnlighterJSRAW" data-enlighter-language="python">num_test_splits</code> you consider, the more likely you&#8217;ll have an accurate shape predictor &#8212; but again, be cautious with this parameter as it can cause training time to explode.</p>
<p>Let&#8217;s check out the <code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_amount</code> next:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="50" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="106"># controls amount of "jitter" (i.e., data augmentation) when training
# the shape predictor -- applies the supplied number of random
# deformations, thereby performing regularization and increasing the
# ability of our model to generalize
options.oversampling_amount = 5
</pre>


<p>The <code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_amount</code> controls the amount of <a href="https://pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/" target="_blank" rel="noopener noreferrer">data augmentation</a> applied to our training data. The dlib library causes data augmentation <strong>jitter</strong>, but it is essentially the same idea as data augmentation.</p>
<p>Here we are telling dlib to apply a total of <code class="EnlighterJSRAW" data-enlighter-language="python">5</code> random deformations to each input image.</p>
<p>You can think of the <code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_amount</code> as a regularization parameter as it may <em>lower</em> training accuracy but <em>increase</em> testing accuracy, thereby allowing our model to generalize better.</p>
<p>Typical <code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_amount</code> values lie in the range <em>[0, 50]</em> where <code class="EnlighterJSRAW" data-enlighter-language="python"> 0</code> means no augmentation and <code class="EnlighterJSRAW" data-enlighter-language="python">50</code> is a 50x increase in your training dataset.</p>
<p><strong>Be careful with this parameter!</strong> Larger <code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_amount</code> values <em>may seem like a good idea</em> but they can <em>dramatically</em> increase your training time.</p>
<p>Next comes the <code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_translation_jitter</code> option:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="56" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="115"># amount of translation jitter to apply -- the dlib docs recommend
# values in the range [0, 0.5]
options.oversampling_translation_jitter = 0.1
</pre>


<p>The <code class="EnlighterJSRAW" data-enlighter-language="python">oversampling_translation_jitter</code>&nbsp;controls the amount of translation augmentation applied to our training dataset.</p>
<p>Typical values for translation jitter lie in the range <em>[0, 0.5].</em></p>
<p>The <code class="EnlighterJSRAW" data-enlighter-language="python">be_verbose</code> option simply instructs dlib to print out status messages as our shape predictor is training:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="60" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="118"># tell the dlib shape predictor to be verbose and print out status
# messages our model trains
options.be_verbose = True
</pre>


<p>Finally, we have the <code class="EnlighterJSRAW" data-enlighter-language="python">num_threads</code> parameter:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="64" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="120"># number of threads/CPU cores to be used when training -- we default
# this value to the number of available cores on the system, but you
# can supply an integer value here if you would like
options.num_threads = multiprocessing.cpu_count()
</pre>


<p>This parameter is <strong><em>extremely important</em></strong> as it can <strong><em>dramatically</em></strong> speed up the time it takes to train your model!</p>
<p>The more CPU threads/cores you can supply to dlib, the faster your model will train. We&#8217;ll default this value to the total number of CPUs on our system; however, you can set this value as any integer (provided it&#8217;s less-than-or-equal-to the number of CPUs on your system).</p>
<p>Now that our <code class="EnlighterJSRAW" data-enlighter-language="python">options</code> are set, the final step is to simply call <code class="EnlighterJSRAW" data-enlighter-language="python">train_shape_predictor</code>:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="69" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="123"># log our training options to the terminal
print("[INFO] shape predictor options:")
print(options)

# train the shape predictor
print("[INFO] training shape predictor...")
dlib.train_shape_predictor(args["training"], args["model"], options)
</pre>


<p>The dlib library accepts (1) the path to our training XML file, (2) the path to our output shape predictor model, and (3) our set of options.</p>
<p>Once trained the shape predictor will be serialized to disk so we can later use it.</p>
<p>While this script may have appeared especially easy, be sure to spend time configuring your options/hyperparameters for optimal performance.</p>
<h3>Training the custom dlib shape predictor</h3>
<p>We are now ready to train our custom dlib shape predictor!</p>
<p>Make sure you have (1) downloaded the iBUG-300W dataset and (2) used the <strong><em>&#8220;Downloads&#8221;</em></strong> section of this tutorial to download the source code to this post.</p>
<p>Once you have done so, you are ready to train the shape predictor:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="124">$ python train_shape_predictor.py \
	--training ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml \
	--model eye_predictor.dat
[INFO] setting shape predictor options...
[INFO] shape predictor options:
shape_predictor_training_options(be_verbose=1, cascade_depth=15, tree_depth=4, num_trees_per_cascade_level=500, nu=0.1, oversampling_amount=5, oversampling_translation_jitter=0.1, feature_pool_size=400, lambda_param=0.1, num_test_splits=50, feature_pool_region_padding=0, random_seed=, num_threads=20, landmark_relative_padding_mode=1)
[INFO] training shape predictor...
Training with cascade depth: 15
Training with tree depth: 4
Training with 500 trees per cascade level.
Training with nu: 0.1
Training with random seed:
Training with oversampling amount: 5
Training with oversampling translation jitter: 0.1
Training with landmark_relative_padding_mode: 1
Training with feature pool size: 400
Training with feature pool region padding: 0
Training with 20 threads.
Training with lambda_param: 0.1
Training with 50 split tests.
Fitting trees...
Training complete
Training complete, saved predictor to file eye_predictor.dat
</pre>


<p>The entire training process took <strong>9m11s</strong> on my 3 GHz Intel Xeon W processor.</p>
<p>To verify that your shape predictor has been serialized to disk, ensure that <code class="EnlighterJSRAW" data-enlighter-language="python">eye_predictor.dat</code> has been created in your directory structure:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="11" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="126">$ ls -lh *.dat
-rw-r--r--@ 1 adrian  staff    18M Dec  4 17:15 eye_predictor.dat
</pre>


<p>As you can see, the output model is only <strong>18MB</strong> &#8212; that&#8217;s quite the reduction in file size compared to dlib&#8217;s standard/default facial landmark predictor which is <strong>99.7MB!</strong></p>
<h3>Implementing our shape predictor evaluation script</h3>
<p>Now that we&#8217;ve trained our dlib shape predictor, we need to evaluate its performance on both our training and testing sets to verify that it&#8217;s not overfitting and that our results will (ideally) generalize to our own images outside the training set.</p>
<p>Open up the <code class="EnlighterJSRAW" data-enlighter-language="shell">evaluate_shape_predictor.py</code> file and insert the following code:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="128"># import the necessary packages
import argparse
import dlib

# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--predictor", required=True,
	help="path to trained dlib shape predictor model")
ap.add_argument("-x", "--xml", required=True,
	help="path to input training/testing XML file")
args = vars(ap.parse_args())

# compute the error over the supplied data split and display it to
# our screen
print("[INFO] evaluating shape predictor...")
error = dlib.test_shape_predictor(args["xml"], args["predictor"])
print("[INFO] error: {}".format(error))
</pre>


<p><strong>Lines 2 and 3</strong> indicate that we need both <code class="EnlighterJSRAW" data-enlighter-language="python">argparse</code>&nbsp; and <code class="EnlighterJSRAW" data-enlighter-language="python">dlib</code>&nbsp; to evaluate our shape predictor.</p>
<p>Our command line arguments include:</p>
<ul>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">--predictor</code>&nbsp;: The path to our serialized shape predictor model that we generated via the previous two <em>&#8220;Training&#8221;</em> sections.</li>
<li><code class="EnlighterJSRAW" data-enlighter-language="python">--xml</code>&nbsp;: The path to the input training/testing XML file (i.e. our <strong>eyes-only</strong> parsed XML files).</li>
</ul>
<p>When both of these arguments are provided via the command line, dlib will handle evaluation (<strong>Line 16</strong>). Dlib handles computing the mean average error (MAE) between the <strong><em>predicted</em></strong> landmark coordinates and the <strong><em>ground-truth</em></strong> landmark coordinates.</p>
<p><strong>The <em>smaller</em> the MAE, the <em>better</em> the predictions.</strong></p>
<h3>Shape prediction accuracy results</h3>
<p>If you haven&#8217;t yet, use the <strong><em>&#8220;Downloads&#8221;</em></strong> section of this tutorial to download the source code and pre-trained shape predictor.</p>
<p>From there, execute the following command to evaluate our eye landmark predictor on the <strong>training set:</strong></p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="133">$ python evaluate_shape_predictor.py --predictor eye_predictor.dat \
	--xml ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml
[INFO] evaluating shape predictor...
[INFO] error: 3.631152776257545
</pre>


<p>Here we are obtaining an <strong>MAE of ~3.63</strong>.</p>
<p>Let&#8217;s now run the same command on our <strong>testing set:</strong></p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="134">$ python evaluate_shape_predictor.py --predictor eye_predictor.dat \
	--xml ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_eyes.xml
[INFO] evaluating shape predictor...
[INFO] error: 7.568211111799696
</pre>


<p>As you can see the <strong>MAE is twice as large</strong> on our testing set versus our training set.</p>
<p>If you have any prior experience working with machine learning or deep learning algorithms you know that in most situations, your training loss will be lower than your testing loss. That doesn&#8217;t mean that your model is performing badly &#8212; instead, it simply means that your model is doing a better job modeling the training data versus the testing data.</p>
<p>Shape predictors are especially interesting to evaluate as <em>it&#8217;s not just the MAE that needs to be examined!</em></p>
<p>You also need to <strong>visually validate</strong> the results and <strong>verify the shape predictor is working as expected</strong> &#8212; we&#8217;ll cover that topic in the next section.</p>
<h3>Implementing the shape predictor inference script</h3>
<p>Now that we have our shape predictor trained, we need to visually validate that the results look good by applying it to our own example images/video.</p>
<p>In this section we will:</p>
<ol>
<li>Load our trained dlib shape predictor from disk.</li>
<li>Access our video stream.</li>
<li>Apply the shape predictor to each individual frame.</li>
<li>Verify that the results look good.</li>
</ol>
<p>Let&#8217;s get started.</p>
<p>Open up <code class="EnlighterJSRAW" data-enlighter-language="shell">predict_eyes.py</code> and insert the following code:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="1" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="136"># import the necessary packages
from imutils.video import VideoStream
from imutils import face_utils
import argparse
import imutils
import time
import dlib
import cv2

# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--shape-predictor", required=True,
	help="path to facial landmark predictor")
args = vars(ap.parse_args())
</pre>


<p><strong>Lines 2-8</strong> import necessary packages. In particular we will use <code class="EnlighterJSRAW" data-enlighter-language="python">imutils</code>&nbsp; and OpenCV (<code class="EnlighterJSRAW" data-enlighter-language="python">cv2</code>) in this script. Our <code class="EnlighterJSRAW" data-enlighter-language="python">VideoStream</code>&nbsp; class will allow us to access our webcam. The <code class="EnlighterJSRAW" data-enlighter-language="python">face_utils</code>&nbsp; module contains a helper function used to convert dlib&#8217;s landmark predictions to a NumPy array.</p>
<p>The only command line argument required for this script is the path to our trained facial landmark predictor, <code class="EnlighterJSRAW" data-enlighter-language="python">--shape-predictor</code>&nbsp;.</p>
<p>Let&#8217;s perform three initializations:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="16" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="142"># initialize dlib's face detector (HOG-based) and then load our
# trained shape predictor
print("[INFO] loading facial landmark predictor...")
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(args["shape_predictor"])

# initialize the video stream and allow the cammera sensor to warmup
print("[INFO] camera sensor warming up...")
vs = VideoStream(src=0).start()
time.sleep(2.0)
</pre>


<p>Our initializations include:</p>
<ul>
<li>Loading the face <code class="EnlighterJSRAW" data-enlighter-language="python">detector</code>&nbsp; (<strong>Line 19</strong>). The detector allows us to find a face in an image/video prior to localizing landmarks on the face. We&#8217;ll be using dlib&#8217;s HOG + Linear SVM face detector. Alternatively, you could use Haar cascades (great for resource-constrained, embedded devices) or a more accurate <a href="https://pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/" target="_blank" rel="noopener noreferrer">deep learning face detector</a>.</li>
<li>Loading the facial landmark <code class="EnlighterJSRAW" data-enlighter-language="python">predictor</code>&nbsp; (<strong>Line 20</strong>).</li>
<li>Initializing our webcam stream (<strong>Line 24</strong>).</li>
</ul>
<p>Now we&#8217;re ready to loop over frames from our camera:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="27" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="145"># loop over the frames from the video stream
while True:
	# grab the frame from the video stream, resize it to have a
	# maximum width of 400 pixels, and convert it to grayscale
	frame = vs.read()
	frame = imutils.resize(frame, width=400)
	gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

	# detect faces in the grayscale frame
	rects = detector(gray, 0)
</pre>


<p><strong>Lines 31-33</strong> grab a frame, resize it, and convert to grayscale.</p>
<p><strong>Line 36</strong> applies face detection using dlib&#8217;s HOG + Linear SVM algorithm.</p>
<p>Let&#8217;s process the faces detected in the frame by <strong>predicting and drawing facial landmarks:</strong></p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="38" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="146">	# loop over the face detections
	for rect in rects:
		# convert the dlib rectangle into an OpenCV bounding box and
		# draw a bounding box surrounding the face
		(x, y, w, h) = face_utils.rect_to_bb(rect)
		cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

		# use our custom dlib shape predictor to predict the location
		# of our landmark coordinates, then convert the prediction to
		# an easily parsable NumPy array
		shape = predictor(gray, rect)
		shape = face_utils.shape_to_np(shape)

		# loop over the (x, y)-coordinates from our dlib shape
		# predictor model draw them on the image
		for (sX, sY) in shape:
			cv2.circle(frame, (sX, sY), 1, (0, 0, 255), -1)
</pre>


<p><strong>Line 39</strong> begins a loop over the detected faces. Inside the loop, we:</p>
<ul>
<li>Take dlib&#8217;s <code class="EnlighterJSRAW" data-enlighter-language="python">rectangle</code> object and convert it to OpenCV&#8217;s standard <code class="EnlighterJSRAW" data-enlighter-language="python">(x, y, w, h)</code> bounding box ordering (<strong>Line 42</strong>).</li>
<li>Draw the bounding box surrounding the face (<strong>Line 43</strong>).</li>
<li>Use our custom dlib shape <code class="EnlighterJSRAW" data-enlighter-language="python">predictor</code>&nbsp; to predict the location of our landmarks (i.e., eyes) via <strong>Line 48</strong>.</li>
<li>Convert the returned coordinates to a NumPy array (<strong>Line 49</strong>).</li>
<li>Loop over the predicted landmark coordinates and draw them individually as small dots on the output frame (<strong>Line 53 and 54</strong>).</li>
</ul>
<p>If you need a refresher on drawing rectangles and solid circles, refer to my <a href="https://pyimagesearch.com/2018/07/19/opencv-tutorial-a-guide-to-learn-opencv/" target="_blank" rel="noopener noreferrer">OpenCV Tutorial</a>.</p>
<p>To wrap up we&#8217;ll display the result!</p>


<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="56" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="150">	# show the frame
	cv2.imshow("Frame", frame)
	key = cv2.waitKey(1) &amp; 0xFF

	# if the `q` key was pressed, break from the loop
	if key == ord("q"):
		break

# do a bit of cleanup
cv2.destroyAllWindows()
vs.stop()
</pre>


<p><strong>Lines 57</strong> displays the frame to the screen.</p>
<p>If the <code class="EnlighterJSRAW" data-enlighter-language="python">q</code>&nbsp; key is pressed at any point while we&#8217;re processing frames from our video stream, we&#8217;ll break and perform cleanup.</p>
<h3>Making predictions with our dlib shape predictor</h3>
<p>Are you ready to see our custom shape predictor in action?</p>
<p>If so, make sure you use the <strong><em>&#8220;Downloads&#8221;</em></strong> section of this tutorial to download the source code and pre-trained dlib shape predictor.</p>
<p>From there you can execute the following command:</p>


<pre class="EnlighterJSRAW" data-enlighter-language="shell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="true" data-enlighter-lineoffset="" data-enlighter-title="Training a custom dlib shape predictor" data-enlighter-group="152">$ python predict_eyes.py --shape-predictor eye_predictor.dat
[INFO] loading facial landmark predictor...
[INFO] camera sensor warming up...
</pre>


<p><script src="https://fast.wistia.com/embed/medias/ptn7l1ysjj.jsonp" async=""></script><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script></p>
<div class="wistia_responsive_padding" style="padding: 61.88% 0 0 0; position: relative;">
<div class="wistia_responsive_wrapper" style="height: 100%; left: 0; position: absolute; top: 0; width: 100%;">
<div class="wistia_embed wistia_async_ptn7l1ysjj videoFoam=true" style="height: 100%; position: relative; width: 100%;"></div>
</div>
</div>
<p>As you can see, our shape predictor is both:</p>
<ul>
<li><strong>Correctly localizing</strong> my eyes in the input video stream</li>
<li><strong>Running in real-time</strong></li>
</ul>
<p>Again, I&#8217;d like to call your attention back to the <em>&#8220;Balancing shape predictor model speed and accuracy&#8221;</em> section of this tutorial &#8212; <strong>our model is not predicting <em>all</em> of the possible 68 landmark locations on the face!</strong></p>
<p>Instead, we have trained a <strong>custom dlib shape predictor that <em>only</em> localizes the eye regions. </strong>(i.e., our model is not trained on the other facial structures in the iBUG-300W dataset including i.e., eyebrows, nose, mouth, and jawline).</p>
<p>Our custom eye predictor can be used in situations where we <em>don&#8217;t need</em> the additional facial structures and <em>only</em> require the eyes, such as <a href="https://pyimagesearch.com/2017/10/23/raspberry-pi-facial-landmarks-drowsiness-detection-with-opencv-and-dlib/" target="_blank" rel="noopener noreferrer">building an a drowsiness detector</a>, building a virtual makeover application for eyeliner/mascara, or creating computer-assisted software to help disabled users utilize their computers.</p>
<p><strong>In next week&#8217;s tutorial, I&#8217;ll show you how to tune the hyperparameters to dlib&#8217;s shape predictor to obtain optimal </strong><strong>performance.</strong></p>
<h3>How do I create my own dataset for shape predictor training?</h3>
<p>To create your own shape predictor dataset you&#8217;ll need to use dlib&#8217;s <a href="https://github.com/davisking/dlib/tree/master/tools/imglab" target="_blank" rel="noopener noreferrer">imglab</a> tool. Covering how to create and annotate your own dataset for shape predictor training is outside the scope of this blog post. I&#8217;ll be covering it in a future tutorial here on PyImageSearch.</p>
<h3>What&#8217;s next?</h3>
<p><a href="https://pyimagesearch.com/pyimagesearch-gurus/" target="_blank" rel="noopener noreferrer"><img class="aligncenter wp-image-10732" src="https://pyimagesearch.com/wp-content/uploads/2019/07/gurus_optin_pitch.png" alt="" width="600" height="346" srcset="https://www.pyimagesearch.com/wp-content/uploads/2019/07/gurus_optin_pitch.png 444w, https://www.pyimagesearch.com/wp-content/uploads/2019/07/gurus_optin_pitch-300x173.png 300w" sizes="(max-width: 600px) 100vw, 600px" /></a></p>
<p>Are you interested in learning more about Computer Vision, OpenCV, and the Dlib library?</p>
<p>If so, you&#8217;ll want to take a look at the <a href="https://pyimagesearch.com/pyimagesearch-gurus/" target="_blank" rel="noopener noreferrer">PyImageSearch Gurus course.</a></p>
<p>Inside&nbsp;<strong>PyImageSearch Gurus</strong>, you&#8217;ll find:</p>
<ul>
<li><strong>An actionable, real-world course on Computer Vision, Deep Learning, and OpenCV.</strong> Each lesson in PyImageSearch Gurus is taught in the same hands-on, easy-to-understand PyImageSearch style that you know and love.</li>
<li><strong>The most&nbsp;<em>comprehensive</em>&nbsp;computer vision education online today</strong>. The PyImageSearch Gurus course covers&nbsp;<strong>13 modules</strong>&nbsp;broken out into&nbsp;<strong>168 lessons</strong>, with other&nbsp;<strong>2,161 pages</strong> of content. You won&#8217;t find a more detailed computer vision course anywhere else online, I guarantee it.</li>
<li><strong>A community of like-minded developers, researchers, and students</strong>&nbsp;just like you, who are eager to learn computer vision and level-up their skills.</li>
<li><strong>Access to private course forums</strong> which I personally participate in nearly every day. These forums are a great way to get expert advice, both from me as well as the more advanced students.</li>
</ul>
<p>To learn more about the course, and grab the course syllabus PDF, just use this link:</p>
<p></p><center><a style="color: #ffffff; text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.247059) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.498039) 0px 1px 3px inset, rgba(0, 0, 0, 0.498039) 0px 1px 3px; background: #DF4B4B;" href="https://pyimagesearch.com/pyimagesearch-gurus/" target="_blank" rel="noopener noreferrer">Send me the course syllabus and 10 <strong><em>free</em></strong> lessons!</a></center><p></p>
<h2>Summary</h2>
<p>In this tutorial, you learned how to train your own custom dlib shape/landmark predictor.</p>
<p>To train our shape predictor we utilized the <a href=

                                                           " target="_blank" rel="noopener noreferrer">iBUG-300W dataset</a>, only instead of training our model to recognize <em>all facial structures</em> (i.e., eyes, eyebrows, nose, mouth, and jawline), <strong>we instead trained the model to localize <em>just the eyes.</em></strong></p>
<p>The end result is a model that is:</p>
<ul>
<li><strong>Accurate:</strong> Our shape predictor can accurately predict/localize the location of the eyes on a face.</li>
<li><strong>Small:</strong> Our eye landmark predictor is smaller than the pre-trained dlib face landmark predictor (18MB vs. 99.7MB, respectively).</li>
<li><strong>Fast:</strong> Our model is faster than dlib&#8217;s pre-trained facial landmark predictor as it predicts fewer locations (the hyperparameters to the model were also chosen to improve speed).</li>
</ul>
<p>In next week&#8217;s tutorial, I&#8217;ll teach you how to systemically tune the hyperparameters to dlib&#8217;s shape predictor training procedure to balance prediction speed, model size, and localization accuracy.</p>
<p><strong>To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), <em>just enter your email address in the form below!</em></strong></p>
<div id="download-the-code" class="post-cta-wrap">
<div class="gpd-post-cta">
	<div class="gpd-post-cta-content">
		

			<div class="gpd-post-cta-top">
				<div class="gpd-post-cta-top-image"><img src="https://www.pyimagesearch.com/wp-content/uploads/2020/01/cta-source-guide-1.png" alt="" /></div>
				
				<div class="gpd-post-cta-top-title"><h4>Download the Source Code and FREE 17-page Resource Guide</h4></div>
				<div class="gpd-post-cta-top-desc"><p>Enter your email address below to get a .zip of the code and a <strong>FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning.</strong> Inside youâ€™ll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!</p></div>


			</div>

			<div class="gpd-post-cta-bottom">
				<form id="footer-cta-code" class="footer-cta" action="https://www.getdrip.com/forms/188555340/submissions" method="post" target="blank" data-drip-embedded-form="188555340">
					<input name="fields[email]" type="email" value="" placeholder="Your email address" class="form-control" />

					<button type="submit">Download the code!</button>

					<div style="display: none;" aria-hidden="true"><label for="website">Website</label><br /><input type="text" id="website" name="website" tabindex="-1" autocomplete="false" value="" /></div>
				</form>
			</div>


		
	</div>

</div>
</div><!-- RightMessage WP -->
<script type="text/javascript">
		window.rmpanda = window.rmpanda || {};
	window.rmpanda.cmsdata = {"cms":"wordpress","postId":11908,"taxonomyTerms":{"category":[418,490,419,561,27],"post_tag":[297,420,563,562],"post_format":[]}};
	</script><!--<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/"
    dc:identifier="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/"
    dc:title="Training a custom dlib shape predictor"
    trackback:ping="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/trackback/" />
</rdf:RDF>-->
</div></article><section class="author-box"><img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=240&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=480&#038;d=mm&#038;r=g 2x' class='avatar avatar-240 photo' height='240' width='240' /><h4 class="author-box-title"><strong>About the Author</strong></h4><div class="author-box-content" itemprop="description"><p>Hi there, I’m Adrian Rosebrock, PhD. All too often I see developers, students, and researchers wasting their time, studying the wrong things, and generally struggling to get started with Computer Vision, Deep Learning, and OpenCV. I created this website to show you what I believe is the best possible way to get your start.</p>
</div></section><h2 class="screen-reader-text">Reader Interactions</h2><div class="single-post-nav"><a href="https://www.pyimagesearch.com/2019/12/09/how-to-install-tensorflow-2-0-on-ubuntu/"><div class="single-post-nav__previous"><p>Previous Article:</p><h3>How to install TensorFlow 2.0 on Ubuntu</h3></div></a><a href="https://www.pyimagesearch.com/2019/12/23/tuning-dlib-shape-predictor-hyperparameters-to-balance-speed-accuracy-and-model-size/"><div class="single-post-nav__next"><p>Next Article:</p><h3>Tuning dlib shape predictor hyperparameters to balance speed, accuracy, and model size</h3></div></a></div><div class="entry-comments" id="comments"><h3>60 responses to: Training a custom dlib shape predictor</h3><ol class="comment-list">
	<li class="comment even thread-even depth-1" id="comment-595649">
	<article id="article-comment-595649">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/ed987085a8e4d012f30a23e7b060a928?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/ed987085a8e4d012f30a23e7b060a928?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Risab Biswas</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-595649">December 16, 2019 at 11:03 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian, Thanks a lot for this Tutorial. I have been looking for this from a long while and now it&#8217;s there in Pyimagesearch and it couldn&#8217;t have been any better. My question is that, if we see the flow of the entire algorithm, It First Detects Face -&gt; And then it detects the Facial Landmarks. The Same is used for your Drowsiness Detection Algo. But I have tested it in scenarios where the Camera is Closer to the face and in those cases Face Detection Fails and eventually the facial landmarks detection also fails. Even if we have a high quality Image or Video the Algorithm doesn&#8217;t seems to work in that situation. Creating a algorithm where it detects only the eye region and then detecting the Eye Landmark points will give the most Optimum Results. Kindly let me know your thoughts. Looking forward to hear from you. Thanks Again for all the Great Work! 🙂</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-595775">
	<article id="article-comment-595775">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-595775">December 16, 2019 at 12:52 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I&#8217;m glad you enjoyed the tutorial, Risab.</p>
<p>To address your question, you are correct that this is a two-stage process &#8212; an object must be detected before any landmarks can be computed.</p>
<p>That said, in your specific example you mentioned the eyes being very close to the camera. In that case, train a very simple HOG + Linear SVM or Haar cascade to detect the eyes, then apply the eye landmark detector to those eye regions.</p>
<p><a target="blank" href="https://www.pyimagesearch.com/practical-python-opencv/" rel="noopener noreferrer">Practical Python and OpenCV</a> includes an eye detector Haar cascade + code that you can use to get yourself started.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment even depth-3" id="comment-602670">
	<article id="article-comment-602670">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/ed987085a8e4d012f30a23e7b060a928?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/ed987085a8e4d012f30a23e7b060a928?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Risab Biswas</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-602670">December 20, 2019 at 4:56 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thanks a lot Adrian for the Reply and the Insights. I will do the things accordingly. </p>
<p>Also if you could tell me that how we can create our own data set for the landmark points it would be great, particularly Can you please suggest an annotation tool for annotating the landmarks? Say for example right now we are only able to detect 6 landmark points around each eyes, but if I would like to get more landmark points, say 10 on each eyes. </p>
<p>Looking forward for your valuable suggestions 🙂</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-4" id="comment-603028">
	<article id="article-comment-603028">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-603028">December 20, 2019 at 7:05 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I would suggest you use dlib&#8217;s &#8220;imglab&#8221; tool for annotation.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment even depth-5" id="comment-603984">
	<article id="article-comment-603984">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/ed987085a8e4d012f30a23e7b060a928?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/ed987085a8e4d012f30a23e7b060a928?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Risab Biswas</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-603984">December 20, 2019 at 1:20 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Great! Thanks Adrian! 🙂</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-5" id="comment-617832">
	<article id="article-comment-617832">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-617832">December 26, 2019 at 9:40 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>You&#8217;re welcome!</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->

	<li class="comment even depth-5" id="comment-696562">
	<article id="article-comment-696562">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/90a92e0e28bf81fa3a671334c2090cc8?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/90a92e0e28bf81fa3a671334c2090cc8?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">jacob lawrence</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-696562">February 3, 2020 at 10:59 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>what if the case was to train a model to detect keypoints over an object on different location in 3d space. In such a situation is it possible to use the same annotation tool? also inorder to obtain a descent accuracy for the model how much data would it require?<br />
is it possible to annotate a 3d object, if it is can you suggest some links or some valuable info to perform such kind of task.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt depth-2" id="comment-627855">
	<article id="article-comment-627855">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/d2838f8f88bab63af0ff4997ea3f388b?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/d2838f8f88bab63af0ff4997ea3f388b?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Omkar Dalvi</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-627855">January 4, 2020 at 11:36 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hey,we are trying a similar project in which we are trying to detect eye motion based on the landmarks. Have you tried the method suggested by Adrian of using eye detector dlib on the close up of eye video in real time. Please let me know if this works. Thanks</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-odd thread-alt depth-1" id="comment-595713">
	<article id="article-comment-595713">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/7a601bff482f00e7933e082f17284cbd?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/7a601bff482f00e7933e082f17284cbd?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Pranav</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-595713">December 16, 2019 at 12:03 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Can it be done in a raspberry pi 3b+</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-595771">
	<article id="article-comment-595771">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-595771">December 16, 2019 at 12:49 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>You can deploy trained dlib shape predictors to the RPi but you cannot realistically train them on the RPi. The RPi is too limited in terms of power and resources.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-even depth-1" id="comment-596042">
	<article id="article-comment-596042">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/9b8c426a3d50db763a651fe33ef3b731?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/9b8c426a3d50db763a651fe33ef3b731?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">David</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-596042">December 16, 2019 at 3:51 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Adrian,</p>
<p>Very thorough guide, I&#8217;ve just been playing with similar stuff with dlib recently. Do you mind me a question on detecting certain shaped objects and fitting landmarks on it&#8230;?</p>
<p>In example a snail from the side view, or a shark even, as you get to look at it from the side it could either be facing left or right.</p>
<p>having trained both left facing and right facing ones as the same object, with landmark points mirrored where necessary, it seems to be mixing it up quite a bit (disabled left right img flips of course)</p>
<p>Would you approach this by pretending the left facing set and its keypoints as one object, and the right facing ones as another calling it snail_a, snail_b or something?</p>
<p>Thanks!</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-598343">
	<article id="article-comment-598343">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-598343">December 18, 2019 at 9:40 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I would do something along the lines of the following:</p>
<p>1. Train a generic &#8220;shark detector&#8221; that can detect left or right views of the shark OR train two detectors, one for left view, one for right view.<br />
2. If you trained two detectors, horizontally flip your the left shark so it always looks like a right shark. If you used a single detector you&#8217;ll need a classifier here to determine the view of the shark.<br />
3. At that point you can apply your landmark predictor.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment even depth-3" id="comment-603742">
	<article id="article-comment-603742">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/9b8c426a3d50db763a651fe33ef3b731?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/9b8c426a3d50db763a651fe33ef3b731?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">David Lipcsey</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-603742">December 20, 2019 at 11:42 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thanks for the clarification, appreciated!<br />
Keep up.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-4" id="comment-617834">
	<article id="article-comment-617834">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-617834">December 26, 2019 at 9:40 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thanks David!</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-odd thread-alt depth-1" id="comment-596401">
	<article id="article-comment-596401">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/aad489b7f414aca7270b357f68dceacf?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/aad489b7f414aca7270b357f68dceacf?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Ajinkya</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-596401">December 17, 2019 at 12:34 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi,</p>
<p>How can I train dlib shape predictor to identify shape of a credit card ?</p>
<p>Regards<br />
Ajinkya</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-598342">
	<article id="article-comment-598342">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-598342">December 18, 2019 at 9:38 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>First you need to create your training set. Your training set should consist of four (x, y)-coordinate pairs which would be the four corners of the credit card itself.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-even depth-1" id="comment-596447">
	<article id="article-comment-596447">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/56ec559b19e65170527ef925677f2a19?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/56ec559b19e65170527ef925677f2a19?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Itai</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-596447">December 17, 2019 at 2:35 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Great tutorial Adrian, thank you so much.<br />
If I can make a request, if you can make more tutorials that leverages also 3D data (such as depth maps / point clouds).<br />
some of us working with depth cameras and would like to know more how can we use the 3D data in a way that will make our RGB model more accurate</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-598340">
	<article id="article-comment-598340">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-598340">December 18, 2019 at 9:37 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thanks for the suggestion, Itai.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-odd thread-alt depth-1" id="comment-597506">
	<article id="article-comment-597506">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/34325e38e8237f1e1ed657ce865207bf?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/34325e38e8237f1e1ed657ce865207bf?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Kevin</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-597506">December 17, 2019 at 9:43 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi, Adrian:</p>
<p>Thanks for the great post, again!</p>
<p>I run into an error while executing train_shape_predictor.py:</p>
<p>&#8230;&#8230;.<br />
Training with lambda_param: 0.1<br />
Training with 50 split tests.<br />
Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.<br />
&#8230;&#8230;.</p>
<p>It seems I missed these two supporting libraries. How to fix this?</p>
<p>Thanks!<br />
Kevin</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->

	<li class="comment odd alt thread-even depth-1" id="comment-597691">
	<article id="article-comment-597691">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/6c58c5c9ccc0a7055fff19862ea1b30e?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/6c58c5c9ccc0a7055fff19862ea1b30e?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Safakat Rahman</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-597691">December 18, 2019 at 2:46 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hello.I am using dlib shape predictor to detect landmarks. But the landmarks are not stable and shaking too much. How to fix that?<br />
Thanx in advance</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-598339">
	<article id="article-comment-598339">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-598339">December 18, 2019 at 9:37 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Take a look at optical flow to help stabilize the landmarks.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-597916">
	<article id="article-comment-597916">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/d360bc2e208e97d7cf23b080a170a2b4?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/d360bc2e208e97d7cf23b080a170a2b4?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Filip Norys</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-597916">December 18, 2019 at 5:29 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Very nice tutorial, thanks for that! I think it would be nice to present how to prepare also you own set of training data in dlib fashion.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-598338">
	<article id="article-comment-598338">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-598338">December 18, 2019 at 9:37 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thanks for the suggestion. It&#8217;s definitely a topic I would like to cover.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-even depth-1" id="comment-617047">
	<article id="article-comment-617047">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/1438f232d1592627c0400e1ea8660ac4?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/1438f232d1592627c0400e1ea8660ac4?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Thanh-Sang Nguyen</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-617047">December 26, 2019 at 3:22 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I wonder if this technique can help us to do the task called &#8220;graph digitizer&#8221;. That means the model can extract numerical of plotted graph with any format. How do you think about it, let me know your opinion. Thank you so much!</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-617838">
	<article id="article-comment-617838">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-617838">December 26, 2019 at 9:41 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>In general I don&#8217;t recommend trying to use computer vision algorithms for that type of task. Try to get access to the raw data instead as it will make processing far easier than trying to use computer vision algorithms to extract the graph data.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-620004">
	<article id="article-comment-620004">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/1438f232d1592627c0400e1ea8660ac4?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/1438f232d1592627c0400e1ea8660ac4?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Thanh-Sang Nguyen</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-620004">December 27, 2019 at 1:05 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thank you for your reply. My task only has image graph instead of raw data, so that I always look for computer vision solution to extract raw data from image graph. So, you really think that this computer vision technique can not help us to extract raw data from graph, aren&#8217;t you? Or if you think it might be possible?</p>
<p>I really appreciate your reply.<br />
Thank you!</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-622483">
	<article id="article-comment-622483">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-622483">January 2, 2020 at 8:42 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>If possible you should try to access the raw data used to generate the graph. Trying to extract lines/plots from a figure can be very challenging and tedious. It&#8217;s far better to go right to the source of the data.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-even depth-1" id="comment-620376">
	<article id="article-comment-620376">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/44963e85b7e331f9c3d950895ff32e3f?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/44963e85b7e331f9c3d950895ff32e3f?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Julien</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-620376">December 29, 2019 at 6:52 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian</p>
<p>First, thanks a lot for all the very nice tutorials which where useful for my project 🙂 </p>
<p>Regarding training, I tried to train the shape predictor by myself, but it seems that the python script consumes too much memory and then the python process is getting killed every time after some while the training script has started. Do you know this problem and do you have any recommendations for solving it?</p>
<p>Cheers</p>
<p>Julien</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-622482">
	<article id="article-comment-622482">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-622482">January 2, 2020 at 8:42 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>See my reply to <a target="blank" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-622433" rel="noopener noreferrer">Ahsan Raza</a></p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-622433">
	<article id="article-comment-622433">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/05a447917f7a7a5c4c057ccfc330847d?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/05a447917f7a7a5c4c057ccfc330847d?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Ahsan Raza</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-622433">January 2, 2020 at 7:18 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian,<br />
I ran into an error</p>
<p>File &#8220;train_shape_predictor.py&#8221;, line 78, in<br />
    dlib.train_shape_predictor(args[&#8220;training&#8221;], args[&#8220;model&#8221;], options)<br />
MemoryError: bad allocation</p>
<p>I have 16gb ram in my system </p>
<p>how can i solve this issue</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-622481">
	<article id="article-comment-622481">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-622481">January 2, 2020 at 8:41 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Your machine doesn&#8217;t have enough RAM, hence the MemoryError. Either:</p>
<p>1. Add more RAM to your machine<br />
2. Decrease the size of the training set</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment odd alt depth-3" id="comment-765511">
	<article id="article-comment-765511">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/863853fb5e35cc263c57a041679fa8dd?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/863853fb5e35cc263c57a041679fa8dd?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">RJ</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-765511">March 4, 2020 at 9:29 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>How do you set the training set for the model? and can you train again a predictor.dat file?</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-4" id="comment-766020">
	<article id="article-comment-766020">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-766020">March 11, 2020 at 4:32 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Refer to this tutorial &#8212; it shows you how to create the training set and supply it to the model.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt depth-3" id="comment-777353">
	<article id="article-comment-777353">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/863853fb5e35cc263c57a041679fa8dd?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/863853fb5e35cc263c57a041679fa8dd?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Rollie John</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-777353">April 11, 2020 at 11:11 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>How can you descrease the batch size of traing like the option batch size in keras so i can use the whole dataset but not run out of memory.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-even depth-1" id="comment-623363">
	<article id="article-comment-623363">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/6c4a6831ed9765e726c240ff9c80012e?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/6c4a6831ed9765e726c240ff9c80012e?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">William Stevenson</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-623363">January 2, 2020 at 5:06 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian</p>
<p>Is there a way to release the camera? My Logitech C920 shows a blue ring around it when it&#8217;s in use. vs.stop() does not free up the camera. Sometimes the system hangs on to the camera and the program will not start again. </p>
<p>PS Most times the code finds my eyes even though I wear glasses.</p>
<p>William</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->

	<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-624064">
	<article id="article-comment-624064">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/05a447917f7a7a5c4c057ccfc330847d?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/05a447917f7a7a5c4c057ccfc330847d?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Ahsan Raza</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-624064">January 3, 2020 at 2:51 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrain,<br />
Thank you for the reply i solved the problem by closing all the other program running<br />
in the system</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-656517">
	<article id="article-comment-656517">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-656517">January 16, 2020 at 10:19 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Congrats on resolving the issue!</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-even depth-1" id="comment-627668">
	<article id="article-comment-627668">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/ced231b37fd84c94ff966471e73ecc53?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/ced231b37fd84c94ff966471e73ecc53?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Mike</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-627668">January 4, 2020 at 7:26 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi! Thanks a lot for this guide! I&#8217;ve noticed that when the face is rotated towards one of the sides so it&#8217;s not  in the front position, model becomes not very accurate. Eyes predictions can not really move along with the face till the end. Do you think this problem can be addressed somehow?<br />
And also what do you think is the minimum  number of training images for the model? I&#8217;ve tried training with like 200 pictures but haven&#8217;t got good results so far.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->

	<li class="comment even thread-odd thread-alt depth-1" id="comment-648478">
	<article id="article-comment-648478">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/531b6f015ea181ff85f5445ef0d16564?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/531b6f015ea181ff85f5445ef0d16564?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">quang nhat tran</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-648478">January 13, 2020 at 9:33 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi, </p>
<p>in my case, the built-in camera on my laptop could not working.<br />
it return line this</p>
<p>[INFO] loading facial landmark predictor&#8230;<br />
[INFO] camera sensor warming up&#8230;<br />
[1]    9169 abort      python predict_eyes.py &#8211;shape-predictor eye_predictor.dat</p>
<p>anyone has that issue ?</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-656513">
	<article id="article-comment-656513">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-656513">January 16, 2020 at 10:19 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Try checking your RAM usage. It sounds like your machine might be running out of RAM.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-even depth-1" id="comment-671222">
	<article id="article-comment-671222">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/fd0c9eb220edb2ecd23f0b073f2c7a36?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/fd0c9eb220edb2ecd23f0b073f2c7a36?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">AI_Developer_OZ</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-671222">January 23, 2020 at 2:53 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian, Appreciations to your works in this stream. Is there any possibilities to annotate human head and combine it with dlib. Like, the detection must include the boundaries of head along with the 68pts. Please share your views on this and suggest any alternative ideas to detect the head part along with face. Thanks in advance.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-671579">
	<article id="article-comment-671579">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-671579">January 23, 2020 at 9:12 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I&#8217;m not sure what you mean by &#8220;boundaries of head&#8221;. Could you elaborate?</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-odd thread-alt depth-1" id="comment-688715">
	<article id="article-comment-688715">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/eef49ffc547da81469f234a7e3404790?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/eef49ffc547da81469f234a7e3404790?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Joel</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-688715">January 30, 2020 at 5:34 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian,</p>
<p>thank you for your amazing tutorials.  I understand the dlib&#8217;s facial predictors are built on Ensemble regression tree structure. What if i want to train my own facial landmark detection using ibug data set and Faster Rcnn or Mobilenet using tensor flow. is it possible? any reference would help. thank you.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-688954">
	<article id="article-comment-688954">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-688954">January 30, 2020 at 8:30 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Yes, it is possible. You would want to look into deep learning for regression models along with deep learning-based keypoint/shape localizers. I&#8217;m covering that in the 4th edition of <a target="blank" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/" rel="noopener noreferrer">Deep Learning for Computer Vision with Python.</a></p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-even depth-1" id="comment-696736">
	<article id="article-comment-696736">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/a7b204057d5edf6fd99b16577b909d9e?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/a7b204057d5edf6fd99b16577b909d9e?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Dexter</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-696736">February 3, 2020 at 12:18 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian<br />
I am a big fan of your work and have the hobbyist bundle for the Rpi. Thank you.</p>
<p>I am trying to follow this tutorial on my MBP (Catalina ) and hit a block when I run </p>
<p>python train_shape_predictor.py<br />
&#8211;training ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml<br />
&#8211;model eye_predictor.dat </p>
<p>RuntimeError: Error, unable to allocate png structure while opening file lfpw/trainset/image_0457.png<br />
This is happening because you compiled against one version of libpng, but then linked to another.<br />
Compiled against libpng version:   1.4.12<br />
Linking to this version of libpng: 1.6.37</p>
<p>I notice that nobody on here has reported hit this problem so I have nothing to go by. I have tried various web searches and suggestions but don&#8217;t know how to re-link to a diff libpng version.<br />
I am using a virtual env created with pip.</p>
<p>Btw, I have run the facial-landmarks (w/ real-time too) tutorials successfully.</p>
<p>I appreciate you are super-busy so apologies for asking you but I am rally interested in this tutorial.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-703677">
	<article id="article-comment-703677">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-703677">February 5, 2020 at 1:41 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I can&#8217;t say I&#8217;ve encountered that error either, Dexter. Unfortunately I&#8217;m not sure what the issue may be as it sounds like the issue is with libpng and not how you installed OpenCV. I would suggest focusing your search on lingpng-related version issues.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-odd thread-alt depth-1" id="comment-759272">
	<article id="article-comment-759272">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/24b6dc42d231796ccb72be4fd2504b69?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/24b6dc42d231796ccb72be4fd2504b69?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">De Hui</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-759272">February 27, 2020 at 7:04 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I get confused at line 53<br />
 for (sX, sY) in shape:<br />
May I know the exact index of each eye landmark so that I can make use of those to calculate the euclidean distance of each eye.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-759284">
	<article id="article-comment-759284">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-759284">February 27, 2020 at 8:58 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>You can refer to <a target="blank" href="https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/" rel="noopener noreferrer">this post</a> which shows the original eye landmark indexes.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-even depth-1" id="comment-765283">
	<article id="article-comment-765283">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/ad79f04f940c3913d4d28966c06b97a2?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/ad79f04f940c3913d4d28966c06b97a2?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Lea</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-765283">March 2, 2020 at 12:32 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hey Adrian, thanks for the tutorial. How could I go about adding additional landmarks for such a dataset? For instance, I want to also be able to detect facial hair.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-765454">
	<article id="article-comment-765454">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-765454">March 4, 2020 at 1:18 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>You would need to annotate your dataset for such landmarks. I would suggest using the &#8220;imglab&#8221; tool included with the dlib library.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment even depth-3" id="comment-766440">
	<article id="article-comment-766440">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/ad79f04f940c3913d4d28966c06b97a2?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/ad79f04f940c3913d4d28966c06b97a2?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Lea</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-766440">March 15, 2020 at 9:17 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thanks for the suggestion. If I have a dataset of individuals with and without facial hair. Will the landmarks for facial hair appear for those who don&#8217;t have facial hair once the model is trained? I want to also be able to add other landmarks for hair detection. What would be the best way to go about doing this in your opinion?</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-765617">
	<article id="article-comment-765617">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/b625773a562fdb7cfc14dfd12304101f?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/b625773a562fdb7cfc14dfd12304101f?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">tigon7476</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-765617">March 5, 2020 at 6:16 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>I am always thankful for your tutorial and wait for it.<br />
Above you said, MAE(Mean Average Error), I searched it at google, I don&#8217;t find that mean.<br />
so I want to know that it means and formula.<br />
Thank you in advance^^</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->

	<li class="comment even thread-even depth-1" id="comment-767391">
	<article id="article-comment-767391">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/fbea6557338b8a386ede937ccb2d8596?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/fbea6557338b8a386ede937ccb2d8596?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Cibya</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-767391">March 23, 2020 at 12:53 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian, Thanks for the tutorial.<br />
I followed all the steps in the tutorial for my dataset which i created using imgLab tool.<br />
When using the dlib.test_shape_predictor it gives error on the training set but for the test set it returns a Nan as the error.<br />
Can you help me?</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-767632">
	<article id="article-comment-767632">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-767632">March 25, 2020 at 1:25 pm</a></time></p>		</header>

		<div class="comment-content">
			
			<p>It&#8217;s hard to diagnose without seeing the exact error message. Was the shape predictor successfully trained?</p>
<p>Secondly, I would assume that the &#8220;not a number&#8221; means that your test set path may be invalid and that no images were able to be evaluated. I would double-check that path.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment even thread-odd thread-alt depth-1" id="comment-770470">
	<article id="article-comment-770470">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/f3bc34078831466f2442ba882f214907?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/f3bc34078831466f2442ba882f214907?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Peter Yin</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-770470">April 5, 2020 at 8:41 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Hi Adrian, </p>
<p>It is a great work to put things together. </p>
<p>A few years ago, We trained a (head top) detector using Dlib along the similar track as described in your tutorial.  However the result is not satisfactory. </p>
<p>Since the dlib algorithm is a two-stage process, and based on the results from our work, we believe that the success of dlib is due to the combination of the both stages. It is hard for the landmark detection alone to do the trick.  </p>
<p>Therefore I personally do not recommend to use dlib landmark detection without knowledge of the object detector to be used in the first stage.  </p>
<p>Please let me know if you are aware of any other object detectors? successfully used as the first stage for the dlib shape predictor.</p>
<p>Thank you</p>
<p>Peter Yin</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor odd alt depth-2" id="comment-772501">
	<article id="article-comment-772501">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-772501">April 9, 2020 at 9:15 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Yes, you really need to treat this as a two stage process. If you can&#8217;t provide the detection to the landmark detector then the landmark detector is not going to provide good results. There is a <a target="blank" href="https://thesai.org/Downloads/Volume10No4/Paper_70-Person_Detection_from_Overhead_View_A_Survey.pdf" rel="noopener noreferrer nofollow ugc">survey paper</a> on overhead person detection, I would suggest starting there for next steps.</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment even depth-3" id="comment-774179">
	<article id="article-comment-774179">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/06751c7e0cb883738748f8fe5ab31bbb?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/06751c7e0cb883738748f8fe5ab31bbb?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Peter</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-774179">April 10, 2020 at 8:15 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thank you for the response.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

	<li class="comment odd alt thread-even depth-1" id="comment-785306">
	<article id="article-comment-785306">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/a44ce25990bc3ff2e1ea9322a8448fd1?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/a44ce25990bc3ff2e1ea9322a8448fd1?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Nikos</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-785306">April 16, 2020 at 6:38 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Good evening. I am trying to train a custom shape detector for detecting landmarks in patients faces, which are deformed. I added 100 more images in the train set (that&#8217;s all i have-manually annotated) but since the normal faces are 6600 and the deformed ones 100 there is a class inbalance. The algorithm fails to detect the landmarks in these patients. Are you aware of any solution? Thank you</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment byuser comment-author-adrian bypostauthor even depth-2" id="comment-785435">
	<article id="article-comment-785435">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Adrian Rosebrock</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-785435">April 16, 2020 at 7:46 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Why not train a shape predictor on <em>just</em> the patients dataset? Is there a particular reason you want to include the other images when training the detector?</p>
		</div>

		
		
	</article>
	<ul class="children">

	<li class="comment odd alt depth-3" id="comment-787200">
	<article id="article-comment-787200">

		
		<header class="comment-header">
			<p class="comment-author">
				<img alt='' src='https://secure.gravatar.com/avatar/83407e99444c09460543eb1f1a7cfdb0?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/83407e99444c09460543eb1f1a7cfdb0?s=96&#038;d=mm&#038;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /><span class="comment-author-name">Nikos</span>			</p>

			<p class="comment-meta"><time class="comment-time"><a class="comment-time-link" href="https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/#comment-787200">April 17, 2020 at 4:36 am</a></time></p>		</header>

		<div class="comment-content">
			
			<p>Thank you for your response. I want to be able to distinguish patients from a control group based on the landmark positions and so, i need them all.</p>
		</div>

		
		
	</article>
	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ol></div><div class="comment-policy">  <h3 class="comment-policy__title">Before you leave a comment...</h3>  <div class="comment-policy__content"><p>Hey, Adrian here, author of the PyImageSearch blog. I&#8217;d love to hear from you; however, I have made the decision to no longer offer free 1:1 help over blog post comments. I simply do not have the time to moderate and respond to them all.</p>
<p><strong>To that end, myself and my team are doubling down our efforts on supporting our paying customers,</strong> writing new books and courses, and authoring high quality Computer Vision, Deep Learning, and OpenCV content for you to learn from.</p>
<p>I&#8217;d be happy to help you with your question or project, but <a href="https://www.pyimagesearch.com/books-and-courses/" target="_blank" rel="noopener noreferrer" data-token-index="1" data-reactroot="">I have to politely ask you to purchase one of my books or courses first.</a></p>
<p>Why bother becoming a PyImageSearch customer?</p>
<ul>
<li>You&#8217;ll receive a <strong>great education</strong> through my premium content.</li>
<li>You&#8217;ll receive <strong>priority support.</strong></li>
<li>You&#8217;ll receive a <strong>guaranteed response </strong>from myself and my team.</li>
<li>You&#8217;ll be able to <strong>confidently and successfully</strong> apply Computer Vision, Deep Learning, and OpenCV to your projects.</li>
</ul>
<p><a href="https://www.pyimagesearch.com/books-and-courses/" target="_blank" rel="noopener noreferrer" data-token-index="0" data-reactroot="">Click here to see my full catalog of books and courses.</a> Take a look and I hope to see you on the other side!</p>
</div></div></main><aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar" id="genesis-sidebar-primary"><h2 class="genesis-sidebar-title screen-reader-text">Primary Sidebar</h2><section id="custom_html-2" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><div class="textwidget custom-html-widget"><div class="sidebar__block">
	<a href="https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/"><img src="https://www.pyimagesearch.com/wp-content/uploads/2020/01/free-crashcourse-200x300-1.jpg" alt="" class="wp-image-11900" width="125" height="188" srcset="https://www.pyimagesearch.com/wp-content/uploads/2020/01/free-crashcourse-200x300-1.jpg 500w, https://www.pyimagesearch.com/wp-content/uploads/2020/01/free-crashcourse-200x300-1.jpg 200w" sizes="(max-width: 125px) 100vw, 125px"></a>	
	<h4 class="sidebar__block-title"><a href="https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/">Free Resource Guide: Computer Vision, OpenCV, and Deep Learning</a></h4>
	<div class="sidebar__block-content">
		<p>Inside you'll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL.</p>	
	</div>
	<a href="https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/" class="button sidebar__block-button">Download</a>
</div></div></div></section>
<section id="custom_html-3" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><div class="textwidget custom-html-widget"><div class="sidebar__block">
	<a target="_blank" href="https://www.pyimagesearch.com/raspberry-pi-for-computer-vision/" rel="noopener noreferrer"><img src="https://www.pyimagesearch.com/wp-content/uploads/2020/01/raspberry-pi-for-computer-vision-200x300-1.jpg" alt="" class="wp-image-11905" width="125" height="188" srcset="https://www.pyimagesearch.com/wp-content/uploads/2020/01/raspberry-pi-for-computer-vision-200x300-1.jpg 500w, https://www.pyimagesearch.com/wp-content/uploads/2020/01/raspberry-pi-for-computer-vision-200x300-1.jpg 200w" sizes="(max-width: 125px) 100vw, 125px"></a>	
	<h4 class="sidebar__block-title"><a target="_blank" href="https://www.pyimagesearch.com/raspberry-pi-for-computer-vision/" rel="noopener noreferrer">Raspberry Pi for Computer Vision</a></h4>
	<div class="sidebar__block-content">
		<p>You can teach your Raspberry Pi to “see” using Computer Vision, Deep Learning, and OpenCV. Let me show you how.</p>	
	</div>
	<a target="_blank" href="https://www.pyimagesearch.com/raspberry-pi-for-computer-vision/" class="sidebar__block-link" rel="noopener noreferrer">Learn More</a>
</div></div></div></section>
<section id="custom_html-4" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><div class="textwidget custom-html-widget"><div class="sidebar__block">
	<a target="_blank" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/" rel="noopener noreferrer"><img src="https://www.pyimagesearch.com/wp-content/uploads/2020/01/deep-learning-for-computer-vision-200x300-1.jpg" alt="" class="wp-image-11907" width="125" height="188" srcset="https://www.pyimagesearch.com/wp-content/uploads/2020/01/deep-learning-for-computer-vision-200x300-1.jpg 500w, https://www.pyimagesearch.com/wp-content/uploads/2020/01/deep-learning-for-computer-vision-200x300-1.jpg 200w" sizes="(max-width: 125px) 100vw, 125px"></a>	
	<h4 class="sidebar__block-title"><a target="_blank" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/" rel="noopener noreferrer">Deep Learning for Computer Vision with Python</a></h4>
	<div class="sidebar__block-content">
		<p>You're interested in deep learning and computer vision, but you don't know how to get started. Let me help. My new book will teach you all you need to know about deep learning.</p>	
	</div>
	<a target="_blank" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/" class="sidebar__block-link" rel="noopener noreferrer">Learn More</a>
</div></div></div></section>
<section id="custom_html-5" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><div class="textwidget custom-html-widget"><div class="sidebar__block">
<h4 class="sidebar__block-title"><a target="_blank" href="https://www.pyimagesearch.com/practical-python-opencv/" rel="noopener noreferrer">You can detect faces in images & video.</a></h4>	
<a target="_blank" href="https://www.pyimagesearch.com/practical-python-opencv/" rel="noopener noreferrer"><img src="https://hcl.pyimagesearch.com/wp-content/uploads/2020/01/detect-faces.jpg" alt="" class="wp-image-11909"></a>	
<div class="sidebar__block-content">
<p>Are you interested in detecting faces in images & video? But tired of Googling for tutorials that never work? Then let me help! I guarantee that my new book will turn you into a face detection ninja by the end of this weekend. Click here to give it a shot yourself.</p>
</div>
<a target="_blank" href="https://www.pyimagesearch.com/practical-python-opencv/" class="sidebar__block-link" rel="noopener noreferrer">Learn More</a>
</div></div></div></section>
<section id="custom_html-6" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><div class="textwidget custom-html-widget"><div class="sidebar__block">
<h4 class="sidebar__block-title"><a target="_blank" href="https://www.pyimagesearch.com/pyimagesearch-gurus/" rel="noopener noreferrer">PyImageSearch Gurus: NOW ENROLLING!</a></h4>	
<a target="_blank" href="https://www.pyimagesearch.com/pyimagesearch-gurus/" rel="noopener noreferrer"><img src="https://www.pyimagesearch.com/wp-content/uploads/2020/01/gurus-course-200x300-1.jpg" alt="" class="wp-image-11903" width="125" height="188" srcset="https://www.pyimagesearch.com/wp-content/uploads/2020/01/gurus-course-200x300-1.jpg 500w, https://www.pyimagesearch.com/wp-content/uploads/2020/01/gurus-course-200x300-1.jpg 200w" sizes="(max-width: 125px) 100vw, 125px"></a>
<div class="sidebar__block-content">
<p>The PyImageSearch Gurus course is now enrolling! Inside the course you'll learn how to perform:</p>
<p>
<strong>
                &middot; Automatic License Plate Recognition (ANPR)<br>&middot; Deep Learning<br>&middot; Face Recognition<br>&middot; <em>...and much more!</em>
</strong>
</p>
<p>
Click the button below to learn more about the course, take a tour, and get 10 (FREE) sample lessons.
</p>
</div>
<a target="_blank" href="https://www.pyimagesearch.com/pyimagesearch-gurus/" class="sidebar__block-link" rel="noopener noreferrer">Learn More</a>
</div></div></div></section>
</aside></div></div></div><div class="similar-articles"><div class="wrap"><h3>Similar articles</h3><div class="gpd-simple-card-group"><article class="post-summary"><a href="https://www.pyimagesearch.com/2015/06/09/i-was-just-interviewed-on-the-talk-python-to-me-podcast/" class="post-summary--link"><header class="entry-header"><div class="entry-categories"><div class="entry-category">Announcements</div></div><h2 class="entry-title">I was just interviewed on the Talk Python to Me Podcast.</h2><div class="entry-date">June 9, 2015</div></header><p class="entry-content-link"><svg class="svg-icon long-arrow" width="14" height="14" aria-hidden="true" role="img" focusable="false" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.8125 0.1875C6.875 0.125 6.96875 0.09375 7.09375 0.09375C7.1875 0.09375 7.28125 0.125 7.34375 0.1875L13.875 6.75C13.9375 6.8125 14 6.90625 14 7C14 7.125 13.9375 7.1875 13.875 7.25L7.34375 13.8125C7.28125 13.875 7.1875 13.9062 7.09375 13.9062C6.96875 13.9062 6.875 13.875 6.8125 13.8125L6.1875 13.1875C6.125 13.125 6.09375 13.0625 6.09375 12.9375C6.09375 12.8438 6.125 12.75 6.1875 12.6562L11.0312 7.8125H0.375C0.25 7.8125 0.15625 7.78125 0.09375 7.71875C0.03125 7.65625 0 7.5625 0 7.4375V6.5625C0 6.46875 0.03125 6.375 0.09375 6.3125C0.15625 6.25 0.25 6.1875 0.375 6.1875H11.0312L6.1875 1.34375C6.125 1.28125 6.09375 1.1875 6.09375 1.0625C6.09375 0.96875 6.125 0.875 6.1875 0.8125L6.8125 0.1875Z" fill="#169FE6"/></svg></p></a></article><article class="post-summary"><a href="https://www.pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/" class="post-summary--link"><header class="entry-header"><div class="entry-categories"><div class="entry-category">Face Applications</div><div class="entry-category">Raspberry Pi</div><div class="entry-category">Tutorials</div></div><h2 class="entry-title">Raspberry Pi Face Recognition</h2><div class="entry-date">June 25, 2018</div></header><p class="entry-content-link"><svg class="svg-icon long-arrow" width="14" height="14" aria-hidden="true" role="img" focusable="false" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.8125 0.1875C6.875 0.125 6.96875 0.09375 7.09375 0.09375C7.1875 0.09375 7.28125 0.125 7.34375 0.1875L13.875 6.75C13.9375 6.8125 14 6.90625 14 7C14 7.125 13.9375 7.1875 13.875 7.25L7.34375 13.8125C7.28125 13.875 7.1875 13.9062 7.09375 13.9062C6.96875 13.9062 6.875 13.875 6.8125 13.8125L6.1875 13.1875C6.125 13.125 6.09375 13.0625 6.09375 12.9375C6.09375 12.8438 6.125 12.75 6.1875 12.6562L11.0312 7.8125H0.375C0.25 7.8125 0.15625 7.78125 0.09375 7.71875C0.03125 7.65625 0 7.5625 0 7.4375V6.5625C0 6.46875 0.03125 6.375 0.09375 6.3125C0.15625 6.25 0.25 6.1875 0.375 6.1875H11.0312L6.1875 1.34375C6.125 1.28125 6.09375 1.1875 6.09375 1.0625C6.09375 0.96875 6.125 0.875 6.1875 0.8125L6.8125 0.1875Z" fill="#169FE6"/></svg></p></a></article><article class="post-summary"><a href="https://www.pyimagesearch.com/2015/02/05/thumbs-hand-gesture-recognition/" class="post-summary--link"><header class="entry-header"><div class="entry-categories"><div class="entry-category">Kickstarter</div></div><h2 class="entry-title">Thumbs up: Hand gesture recognition.</h2><div class="entry-date">February 5, 2015</div></header><p class="entry-content-link"><svg class="svg-icon long-arrow" width="14" height="14" aria-hidden="true" role="img" focusable="false" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.8125 0.1875C6.875 0.125 6.96875 0.09375 7.09375 0.09375C7.1875 0.09375 7.28125 0.125 7.34375 0.1875L13.875 6.75C13.9375 6.8125 14 6.90625 14 7C14 7.125 13.9375 7.1875 13.875 7.25L7.34375 13.8125C7.28125 13.875 7.1875 13.9062 7.09375 13.9062C6.96875 13.9062 6.875 13.875 6.8125 13.8125L6.1875 13.1875C6.125 13.125 6.09375 13.0625 6.09375 12.9375C6.09375 12.8438 6.125 12.75 6.1875 12.6562L11.0312 7.8125H0.375C0.25 7.8125 0.15625 7.78125 0.09375 7.71875C0.03125 7.65625 0 7.5625 0 7.4375V6.5625C0 6.46875 0.03125 6.375 0.09375 6.3125C0.15625 6.25 0.25 6.1875 0.375 6.1875H11.0312L6.1875 1.34375C6.125 1.28125 6.09375 1.1875 6.09375 1.0625C6.09375 0.96875 6.125 0.875 6.1875 0.8125L6.8125 0.1875Z" fill="#169FE6"/></svg></p></a></article></div></div></div><div class="gpd-footer-cta"><div class="wrap"><div class="footer-cta-grid"><div class="footer-cta-image"><img width="932" height="833" src="https://www.pyimagesearch.com/wp-content/uploads/2020/02/man-on-sofa-with-laptop-1.jpg" class="attachment-full size-full" alt="" srcset="https://www.pyimagesearch.com/wp-content/uploads/2020/02/man-on-sofa-with-laptop-1.jpg 932w, https://www.pyimagesearch.com/wp-content/uploads/2020/02/man-on-sofa-with-laptop-1-300x268.jpg 300w, https://www.pyimagesearch.com/wp-content/uploads/2020/02/man-on-sofa-with-laptop-1-768x686.jpg 768w" sizes="(max-width: 932px) 100vw, 932px" /></div><div class="footer-cta-title"><h3>You can learn Computer Vision, Deep Learning, and OpenCV.</h3></div><div class="footer-cta-content"><div class="footer-cta-content-desc"><p>Get your FREE 17 page Computer Vision, OpenCV, and Deep Learning Resource Guide PDF. Inside you&#8217;ll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL.</p>
</div><div class="footer-cta-content-action"><form class="footer-cta" action="https://www.getdrip.com/forms/657075648/submissions" method="post" target="_blank" data-drip-embedded-form="657075648">
	<input type="email" name="fields[email]" class="form-control" id="email" value="" placeholder="Your email address"/>
	<button type="submit" data-drip-attribute="sign-up-button">Download for free</button>
	<div style="display: none;" aria-hidden="true"><label for="website">Website</label><br /><input type="text" id="website" name="website" tabindex="-1" autocomplete="false" value="" /></div>
</form></div></div></div></div></div><div class="footer-widgets" id="genesis-footer-widgets"><h2 class="genesis-sidebar-title screen-reader-text">Footer</h2><div class="wrap"><div class="widget-area footer-widgets-1 footer-widget-area"><section id="custom_html-7" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><h3 class="widgettitle widget-title">Topics</h3>
<div class="textwidget custom-html-widget"><ul>
	<li><a href="https://www.pyimagesearch.com/category/deep-learning-2/">Deep Learning</a></li>
	<li><a href="https://www.pyimagesearch.com/category/dlib/">Dlib Library</a></li>
	<li><a href="https://www.pyimagesearch.com/category/embedded/">Embedded/IoT and Computer Vision</a></li>
	<li><a href="https://www.pyimagesearch.com/category/faces/">Face Applications</a></li>
	<li><a href="https://www.pyimagesearch.com/category/image-processing/">Image Processing</a></li>
	<li><a href="https://www.pyimagesearch.com/category/interviews/">Interviews</a></li>
	<li><a href="https://www.pyimagesearch.com/category/keras/">Keras</a></li>
</ul>
</div></div></section>
</div><div class="widget-area footer-widgets-2 footer-widget-area"><section id="custom_html-8" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><div class="textwidget custom-html-widget"><ul>
	<li><a href="https://www.pyimagesearch.com/category/machine-learning-2/">Machine Learning and Computer Vision</a></li>
	<li><a href="https://www.pyimagesearch.com/category/medical/">Medical Computer Vision</a></li>
	<li><a href="https://www.pyimagesearch.com/category/optical-character-recognition-ocr/">Optical Character Recognition (OCR)</a></li>
	<li><a href="https://www.pyimagesearch.com/category/object-detection/">Object Detection</a></li>
	<li><a href="https://www.pyimagesearch.com/category/object-tracking/">Object Tracking</a></li>
	<li><a href="https://www.pyimagesearch.com/category/opencv/">OpenCV Tutorials</a></li>
	<li><a href="https://www.pyimagesearch.com/category/raspberry-pi/">Raspberry Pi</a></li>
</ul></div></div></section>
</div><div class="widget-area footer-widgets-3 footer-widget-area"><section id="custom_html-9" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><h3 class="widgettitle widget-title">Books &#038; Courses</h3>
<div class="textwidget custom-html-widget"><ul>
<li><a href="https://www.pyimagesearch.com/free-opencv-computer-vision-deep-learning-crash-course/">FREE CV, DL, and OpenCV Crash Course</a></li>
<li><a href="https://www.pyimagesearch.com/practical-python-opencv/">Practical Python and OpenCV</a></li>
<li><a href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python</a></li>
<li><a href="https://www.pyimagesearch.com/pyimagesearch-gurus/">PyImageSearch Gurus Course</a></li>
<li><a href="https://www.pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision</a></li>
</ul></div></div></section>
</div><div class="widget-area footer-widgets-4 footer-widget-area"><section id="custom_html-10" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><h3 class="widgettitle widget-title">PyImageSearch</h3>
<div class="textwidget custom-html-widget"><ul>
	<li><a href="https://www.pyimagesearch.com/start-here/">Get Started</a></li>
	<li><a href="https://www.pyimagesearch.com/opencv-tutorials-resources-guides/">OpenCV Install Guides</a></li>
	<li><a href="https://www.pyimagesearch.com/about/">About</a></li>
	<li><a href="https://www.pyimagesearch.com/faqs/">FAQ</a></li>
	<li><a href="https://www.pyimagesearch.com/topics/">Blog</a></li>
	<li><a href="https://www.pyimagesearch.com/contact/">Contact</a></li>
	<li><a href="https://www.pyimagesearch.com/privacy-policy/">Privacy Policy</a></li>
</ul></div></div></section>
</div></div></div><footer class="site-footer"><div class="wrap"><div class="footer-logo"><p class="site-title"><a href="https://www.pyimagesearch.com"></a></p></div><div class="footer-social"><a target="_blank" href="https://www.facebook.com/pyimagesearch"><svg class="svg-icon social-icon" width="18" height="18" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 264 512"><path d="M215.8 85H264V3.6C255.7 2.5 227.1 0 193.8 0 124.3 0 76.7 42.4 76.7 120.3V192H0v91h76.7v229h94V283h73.6l11.7-91h-85.3v-62.7c0-26.3 7.3-44.3 45.1-44.3z"/></svg></a><a target="_blank" href="https://twitter.com/PyImageSearch"><svg class="svg-icon social-icon" width="18" height="18" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a><a target="_blank" href="http://www.linkedin.com/pub/adrian-rosebrock/2a/873/59b"><svg class="svg-icon social-icon" width="18" height="18" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a><a target="_blank" href="https://www.youtube.com/channel/UCoQK7OVcIVy-nV4m-SMCk_Q/videos"><svg class="svg-icon social-icon" width="18" height="18" aria-hidden="true" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg></a></div><div class="footer-info">&copy; 2020 <a href="https://www.pyimagesearch.com">PyImageSearch</a>. All Rights Reserved.</div></div></footer></div>		<div id="pyi-pyimagesearch-plus-optin-modal" class="front-page-modal modal">
			<div class="front-modal-top">
				<h3>Get <em>instant access</em> to the code for this tutorial and all other 400 tutorials on PyImageSearch.</h3>
			</div>

			<div class="front-modal-video" style="margin-top: 50px;">
				<div class="front-modal-video-embed">
					<img src="https://www.pyimagesearch.com/wp-content/uploads/2020/09/library_logos.png" alt=""/>
				</div>
				<div class="front-modal-video-desc">
					<p>Inside you'll find...</p>
					<ul class="is-style-list-checks">
						<li>Access to <strong>centralized code repositories</strong> for <strong><em>all</em> 400 tutorials</strong> on the PyImageSearch blog</li>
						<li>Easily access code for <strong>all <em>new</em> tutorials</strong> that publish every Monday at 10AM EST</li>
						<li><strong>Easy one-click downloads</strong> for source code, datasets, pre-trained models, etc.</li>
					</ul>
				</div>
			</div>

			<div class="front-modal-action">
				<p><strong>Enter your email address below get access:</strong></p>
				<form class="footer-cta" action="https://www.getdrip.com/forms/857913265/submissions" method="post" target="_blank" data-drip-embedded-form="857913265" id="drip-ef-857913265">
					<input type="email" name="fields[email]" class="form-control" id="email" value="" placeholder="Your email address"/>
					<input id="code_submit_post_title" type="hidden" name="fields[code_submit_post_title]" value="" />
					<button type="submit" data-drip-attribute="sign-up-button">Get Access!</button>
					<div style="display: none;" aria-hidden="true"><label for="website">Website</label><br /><input type="text" id="website" name="website" tabindex="-1" autocomplete="false" value="" /></div>
				</form>
			</div>

			<div class="front-modal-testimonial">
				<blockquote><p>I used part of one of your tutorials to solve Python and OpenCV issue I was having. Struggled with it for two weeks with no answer from other websites experts. Read your article I found .... Fixed it in two hours. And it was mission critical too. Your stuff is quality!</p><cite><span class="quote-name">Ismail Thomas-Benge</span><span class="cite-title">Senior QA Consultant and Architect</span></cite></blockquote>
			</div>
		</div>

		<div id="pyi-pyimagesearch-plus-pricing-modal" class="front-page-modal modal">
			<div class="front-modal-top">
				<h3>PyImageSearch Plus - Get <em>instant access</em> to the code for this tutorial and all other 400 tutorials on PyImageSearch</h3>
				<p>$4.95/month</p>
			</div>

			<div class="front-modal-video">
				<div class="front-modal-video-embed">
					<script src="https://fast.wistia.com/embed/medias/zzmr9wotoj.jsonp" async></script><script src="https://fast.wistia.com/assets/external/E-v1.js" async></script><div class="wistia_responsive_padding" style="padding:62.5% 0 0 0;position:relative;"><div class="wistia_responsive_wrapper" style="height:100%;left:0;position:absolute;top:0;width:100%;"><div class="wistia_embed wistia_async_zzmr9wotoj videoFoam=true" style="height:100%;position:relative;width:100%"><div class="wistia_swatch" style="height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;"><img src="https://fast.wistia.com/embed/medias/zzmr9wotoj/swatch" style="filter:blur(5px);height:100%;object-fit:contain;width:100%;" alt="" aria-hidden="true" onload="this.parentNode.style.opacity=1;" /></div></div></div></div>
				</div>
				<div class="front-modal-video-desc">
					<p>PyImageSearch Plus includes...</p>
					<ul class="is-style-list-checks">
						<li>Access to <strong>centralized code repositories</strong> for <strong><em>all</em> 400 tutorials</strong> on the PyImageSearch blog</li>
						<li>Easily access code for <strong>all <em>new</em> tutorials</strong> that publish every Monday at 10AM EST</li>
						<li><strong>Easy one-click downloads</strong> for source code, datasets, pre-trained models, etc.</li>
						<li>Cancel anytime</li>
					</ul>
				</div>
			</div>

			<div class="front-modal-action">
				<a class="button link" href="https://pyimagesearch-plus.dpdcart.com/subscriber/add?plan_id=1149&#038;plan_term_id=2085" style="background-color: #6DC713; border-bottom: none;">Join for $4.95/month</a>
				<p><a href="https://pyimagesearch-plus.dpdcart.com/subscriber/add?plan_id=1149&plan_term_id=2086">Or, go annual for $49.50/year and save 15%!</a></p>
			</div>

			<div class="front-modal-testimonial">
			<blockquote><p>I used part of one of your tutorials to solve Python and OpenCV issue I was having. Struggled with it for two weeks with no answer from other websites experts. Read your article I found .... Fixed it in two hours. And it was mission critical too. Your stuff is quality!</p><cite><span class="quote-name">Ismail Thomas-Benge</span><span class="cite-title">Senior QA Consultant and Architect</span></cite></blockquote>
			</div>
		</div><!-- RightMessage -->
<script type="text/javascript"> 
(function(p, a, n, d, o, b) {
    o = n.createElement('script'); o.type = 'text/javascript'; o.async = true; o.src = 'https://tag.rightmessage.com/'+p+'.js';
    b = n.getElementsByTagName('script')[0]; b.parentNode.insertBefore(o, b);
    d = function(h, u, i) { var o = n.createElement('style'); o.id = 'rmcloak'+i; o.type = 'text/css';
        o.appendChild(n.createTextNode('.rmcloak'+h+'{visibility:hidden}.rmcloak'+u+'{display:none}'));
        b.parentNode.insertBefore(o, b); return o; }; o = d('', '-hidden', ''); d('-stay-invisible', '-stay-hidden', '-stay');
    setTimeout(function() { o.parentNode && o.parentNode.removeChild(o); }, a);
})('1871593262', 2500, document);
</script><!-- Drip -->
<script type="text/javascript">
  var _dcq = _dcq || [];
  var _dcs = _dcs || {}; 
  _dcs.account = '4768429';
  
  (function() {
    var dc = document.createElement('script');
    dc.type = 'text/javascript'; dc.async = true; 
    dc.src = '//tag.getdrip.com/4768429.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(dc, s);
  })();
</script>

<!-- facebook -->
<script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window, document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '1465896023527386');
  fbq('track', 'PageView');
</script>
<noscript><img height="1" width="1" style="display:none"
  src="https://www.facebook.com/tr?id=1465896023527386&ev=PageView&noscript=1"
/></noscript>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//web.archive.org/web/20170924062416/https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-46641058-1', 'pyimagesearch.com');
  ga('send', 'pageview');
</script>

<! -- Clicky -->
<script src="//static.getclicky.com/js" type="text/javascript"></script>
<script type="text/javascript">try{ clicky.init(101083980); }catch(e){}</script><!-- This site is converting visitors into subscribers and customers with OptinMonster - https://optinmonster.com :: Campaign Title: Google Colab Access --><script>(function(d){var s=d.createElement('script');s.type='text/javascript';s.src='https://a.omappapi.com/app/js/api.min.js';s.async=true;s.dataset.campaign='gqqsykt4ghad3d5cvado';s.dataset.user='18464';d.getElementsByTagName('head')[0].appendChild(s);})(document);</script><!-- / OptinMonster --><!-- This site is converting visitors into subscribers and customers with OptinMonster - https://optinmonster.com :: Campaign Title: CS: Template --><script>(function(d){var s=d.createElement('script');s.type='text/javascript';s.src='https://a.omappapi.com/app/js/api.min.js';s.async=true;s.dataset.campaign='tortsem7qkvyuxc4cyfi';s.dataset.user='18464';d.getElementsByTagName('head')[0].appendChild(s);})(document);</script><!-- / OptinMonster --><!-- This site is converting visitors into subscribers and customers with OptinMonster - https://optinmonster.com :: Campaign Title: CVDL Resource Guide --><script>(function(d){var s=d.createElement('script');s.type='text/javascript';s.src='https://a.omappapi.com/app/js/api.min.js';s.async=true;s.dataset.campaign='mdoijtrmex7bpm0rp2hn';s.dataset.user='18464';d.getElementsByTagName('head')[0].appendChild(s);})(document);</script><!-- / OptinMonster --><div style="position:absolute;overflow:hidden;clip:rect(0 0 0 0);height:1px;width:1px;margin:-1px;padding:0;border:0"><div class="omapi-shortcode-helper">[email]</div><div class="omapi-shortcode-parsed omapi-encoded">[email]</div></div>		<script type="text/javascript">var gqqsykt4ghad3d5cvado_shortcode = true;var tortsem7qkvyuxc4cyfi_shortcode = true;var mdoijtrmex7bpm0rp2hn_shortcode = true;</script>
		<script type='text/javascript'>
/* <![CDATA[ */
var wpcf7 = {"apiSettings":{"root":"https:\/\/www.pyimagesearch.com\/wp-json\/contact-form-7\/v1","namespace":"contact-form-7\/v1"},"cached":"1"};
/* ]]> */
</script>

<script src="https://www.pyimagesearch.com/wp-content/cache/minify/ec843.js"></script>

<script type='text/javascript' src='https://www.google.com/recaptcha/api.js?render=6LcSHsQUAAAAAIzvikURE5e1jZ-YAGgyhpZnfS6o&#038;ver=3.0'></script>
<script src="https://www.pyimagesearch.com/wp-content/cache/minify/707d8.js"></script>

<script type='text/javascript'>
/* <![CDATA[ */
var pyISOptinMonster = {"post_title":"Training a custom dlib shape predictor","campaign_id":"tortsem7qkvyuxc4cyfi","form_image":"https:\/\/s3-us-west-2.amazonaws.com\/static.pyimagesearch.com\/optins\/cs_custom_dlib_shape_pred.png","drip_form_submission_url":"https:\/\/www.getdrip.com\/forms\/188555340\/submissions","drip_id":"188555340"};
/* ]]> */
</script>






<script src="https://www.pyimagesearch.com/wp-content/cache/minify/95d11.js"></script>

<script type='text/javascript'>
!function(n,o){"undefined"!=typeof EnlighterJS?(n.EnlighterJSINIT=function(){EnlighterJS.init("pre.EnlighterJSRAW", "code.EnlighterJSRAW", {"indent":-1,"ampersandCleanup":true,"linehover":false,"rawcodeDbclick":false,"textOverflow":"break","linenumbers":true,"theme":"pyis-enlighter-theme","retainCssClasses":false})})():(o&&(o.error||o.log)||function(){})("Error: EnlighterJS resources not loaded yet!")}(window,console);
</script>

<script src="https://www.pyimagesearch.com/wp-content/cache/minify/69a5c.js"></script>

<script type="text/javascript">
( function( grecaptcha, sitekey, actions ) {

	var wpcf7recaptcha = {

		execute: function( action ) {
			grecaptcha.execute(
				sitekey,
				{ action: action }
			).then( function( token ) {
				var forms = document.getElementsByTagName( 'form' );

				for ( var i = 0; i < forms.length; i++ ) {
					var fields = forms[ i ].getElementsByTagName( 'input' );

					for ( var j = 0; j < fields.length; j++ ) {
						var field = fields[ j ];

						if ( 'g-recaptcha-response' === field.getAttribute( 'name' ) ) {
							field.setAttribute( 'value', token );
							break;
						}
					}
				}
			} );
		},

		executeOnHomepage: function() {
			wpcf7recaptcha.execute( actions[ 'homepage' ] );
		},

		executeOnContactform: function() {
			wpcf7recaptcha.execute( actions[ 'contactform' ] );
		},

	};

	grecaptcha.ready(
		wpcf7recaptcha.executeOnHomepage
	);

	document.addEventListener( 'change',
		wpcf7recaptcha.executeOnContactform, false
	);

	document.addEventListener( 'wpcf7submit',
		wpcf7recaptcha.executeOnHomepage, false
	);

} )(
	grecaptcha,
	'6LcSHsQUAAAAAIzvikURE5e1jZ-YAGgyhpZnfS6o',
	{"homepage":"homepage","contactform":"contactform"}
);
</script>
		<script type="text/javascript">var omapi_localized = { ajax: 'https://www.pyimagesearch.com/wp-admin/admin-ajax.php?optin-monster-ajax-route=1', nonce: 'ec2966a33f', slugs: {"gqqsykt4ghad3d5cvado":{"slug":"gqqsykt4ghad3d5cvado","mailpoet":false},"tortsem7qkvyuxc4cyfi":{"slug":"tortsem7qkvyuxc4cyfi","mailpoet":false},"mdoijtrmex7bpm0rp2hn":{"slug":"mdoijtrmex7bpm0rp2hn","mailpoet":false}} };</script>
				<script type="text/javascript">var omapi_data = {"wc_cart":[],"object_id":11908,"object_key":"post","object_type":"post","term_ids":[418,490,419,561,27,297,420,563,562]};</script>
		</body></html>

<!--
Performance optimized by W3 Total Cache. Learn more: https://www.w3-edge.com/products/

Object Caching 215/246 objects using disk
Page Caching using disk: enhanced 
Minified using disk
Database Caching 4/29 queries in 0.033 seconds using disk

Served from: www.pyimagesearch.com @ 2020-09-24 08:36:48 by W3 Total Cache
-->